{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4496f51e-8202-4fd3-89d2-92aeca5fab51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¸ Image Caption Viewer Project\n",
    "\n",
    "## âœ… Objective\n",
    "Display a collection of local images with their respective captions from a file. Useful for reviewing datasets, galleries, or annotations.\n",
    "\n",
    "## ğŸ“ Folder Structure\n",
    "- `images/`: contains all the image files\n",
    "- `captions.txt`: contains image filename + caption in format `filename.jpg|caption`\n",
    "- `image_viewer.ipynb`: this notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303f71ca-d518-47c6-b9ed-79c939d51928",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow opencv-python matplotlib numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b41d26-c08d-42e7-991e-79b8b91e014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ecf61d-2f11-410f-8d69-7c6e7c9d6c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd98a793-6b52-4de1-98e8-f4ba2ece565f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the uploaded image\n",
    "image = cv2.imread(\"photo_2025-06-09_21-10-49.jpg\")\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Resize for model input\n",
    "image_resized = cv2.resize(image, (256, 256)) / 255.0\n",
    "image_resized = np.expand_dims(image_resized, axis=0)\n",
    "\n",
    "# Show the original image\n",
    "plt.imshow(image)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f151a69d-3861-4110-8c62-46c5a1b77c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install torch torchvision matplotlib pillow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095088c3-cc71-439e-9cec-7fefdd801183",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e99424-82f3-4e0b-9da0-c55ef3f057a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5771e7-5bd7-4405-b0cd-f03ebc5abb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the pretrained DeepLabV3 model\n",
    "model = models.segmentation.deeplabv3_resnet101(pretrained=True).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43c1b33-8a45-4fd3-9480-28228b6ef05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17e9e25-5553-4a79-956f-de5fec631a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained weights (recommended way)\n",
    "weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "\n",
    "# Load the model with these weights and set to eval mode\n",
    "model = deeplabv3_resnet101(weights=weights)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0560f0-01e8-4c66-b9c7-82107a08629b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your image (replace 'your_image.jpg' with your file)\n",
    "img_path = 'photo_2025-06-09_21-10-49.jpg'\n",
    "input_image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "# Get the preprocessing transform from the weights metadata\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# Preprocess the image: resize, normalize, convert to tensor, etc.\n",
    "input_tensor = preprocess(input_image)\n",
    "\n",
    "# The model expects a batch, so add a batch dimension\n",
    "input_batch = input_tensor.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66387026-4199-46c9-9d5f-f95f4cfc4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():  # No need to track gradients during inference\n",
    "    output = model(input_batch)['out'][0]  # Output is a dict with key 'out'\n",
    "\n",
    "# The output has shape [num_classes, H, W]\n",
    "print(output.shape)  # e.g., torch.Size([21, 480, 480])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c118cbe-7afd-445d-a0f0-91a543ca8c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the predicted class per pixel by taking argmax\n",
    "output_predictions = output.argmax(0).byte().cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff7dc1c-59cc-4fc4-9459-ca451349f7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the class colors (manually defined palette since weights.meta[\"palette\"] gives KeyError)\n",
    "palette = [\n",
    "    (0, 0, 0),        # 0=background\n",
    "    (128, 0, 0),      # 1=aeroplane\n",
    "    (0, 128, 0),      # 2=bicycle\n",
    "    (128, 128, 0),    # 3=bird\n",
    "    (0, 0, 128),      # 4=boat\n",
    "    (128, 0, 128),    # 5=bottle\n",
    "    (0, 128, 128),    # 6=bus\n",
    "    (128, 128, 128),  # 7=car\n",
    "    (64, 0, 0),       # 8=cat\n",
    "    (192, 0, 0),      # 9=chair\n",
    "    (64, 128, 0),     # 10=cow\n",
    "    (192, 128, 0),    # 11=dining table\n",
    "    (64, 0, 128),     # 12=dog\n",
    "    (192, 0, 128),    # 13=horse\n",
    "    (64, 128, 128),   # 14=motorbike\n",
    "    (192, 128, 128),  # 15=person\n",
    "    (0, 64, 0),       # 16=potted plant\n",
    "    (128, 64, 0),     # 17=sheep\n",
    "    (0, 192, 0),      # 18=sofa\n",
    "    (128, 192, 0),    # 19=train\n",
    "    (0, 64, 128)      # 20=tv/monitor\n",
    "]\n",
    "\n",
    "# Create a color mask using the palette\n",
    "color_mask = np.zeros((output_predictions.shape[0], output_predictions.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "for label, color in enumerate(palette):\n",
    "    color_mask[output_predictions == label] = color\n",
    "\n",
    "# Plot original image and mask overlay\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(input_image)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Segmentation Mask\")\n",
    "plt.imshow(color_mask)\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(input_image)\n",
    "plt.imshow(color_mask, alpha=0.5)  # overlay with transparency\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa4af9e-f2c6-46a9-adc9-23f64c087483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC color palette for 21 classes (including background)\n",
    "palette = [\n",
    "    (0, 0, 0),        # 0=background\n",
    "    (128, 0, 0),      # 1=aeroplane\n",
    "    (0, 128, 0),      # 2=bicycle\n",
    "    (128, 128, 0),    # 3=bird\n",
    "    (0, 0, 128),      # 4=boat\n",
    "    (128, 0, 128),    # 5=bottle\n",
    "    (0, 128, 128),    # 6=bus\n",
    "    (128, 128, 128),  # 7=car\n",
    "    (64, 0, 0),       # 8=cat\n",
    "    (192, 0, 0),      # 9=chair\n",
    "    (64, 128, 0),     # 10=cow\n",
    "    (192, 128, 0),    # 11=diningtable\n",
    "    (64, 0, 128),     # 12=dog\n",
    "    (192, 0, 128),    # 13=horse\n",
    "    (64, 128, 128),   # 14=motorbike\n",
    "    (192, 128, 128),  # 15=person\n",
    "    (0, 64, 0),       # 16=potted plant\n",
    "    (128, 64, 0),     # 17=sheep\n",
    "    (0, 192, 0),      # 18=sofa\n",
    "    (128, 192, 0),    # 19=train\n",
    "    (0, 64, 128)      # 20=tv/monitor\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a98595a-79c7-4fd5-ab4c-ad8a089022e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use VOC palette since 'palette' key is missing\n",
    "palette = [\n",
    "    (0, 0, 0), (128, 0, 0), (0, 128, 0), (128, 128, 0), (0, 0, 128),\n",
    "    (128, 0, 128), (0, 128, 128), (128, 128, 128), (64, 0, 0), (192, 0, 0),\n",
    "    (64, 128, 0), (192, 128, 0), (64, 0, 128), (192, 0, 128), (64, 128, 128),\n",
    "    (192, 128, 128), (0, 64, 0), (128, 64, 0), (0, 192, 0), (128, 192, 0),\n",
    "    (0, 64, 128)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca81209-859d-49a1-b605-8cf8569052a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty color mask (same height & width as the output, 3 channels for RGB)\n",
    "color_mask = np.zeros((output_predictions.shape[0], output_predictions.shape[1], 3), dtype=np.uint8)\n",
    "\n",
    "# Color each pixel using the corresponding class color from the palette\n",
    "for label, color in enumerate(palette):\n",
    "    color_mask[output_predictions == label] = color\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371ec468-f262-4209-8883-933840d6e765",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot original image, mask, and overlay\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Original Image\")\n",
    "plt.imshow(input_image)\n",
    "plt.axis('off')\n",
    "\n",
    "# Segmentation Mask\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Segmentation Mask\")\n",
    "plt.imshow(color_mask)\n",
    "plt.axis('off')\n",
    "\n",
    "# Overlay (original + segmentation)\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Overlay\")\n",
    "plt.imshow(input_image)\n",
    "plt.imshow(color_mask, alpha=0.6)  # semi-transparent mask\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1301ae-e459-4cbe-8282-5f06fd2c1e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aed026-5dd4-48ec-9adc-38e4af34561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(color_mask).save(\"segmentation_mask.png\")\n",
    "print(\"Segmentation mask saved as segmentation_mask.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6e35d-4e5b-48b6-ac95-b2b1469a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pillow tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8630ad-118c-4216-bf48-53ae7798487c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image, ImageEnhance\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "class DeepLabSegmentation:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        self.model = deeplabv3_resnet101(pretrained=True).to(self.device).eval()\n",
    "        self.preprocess = T.Compose([\n",
    "            T.Resize((520, 520)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=(0.485, 0.456, 0.406),\n",
    "                        std=(0.229, 0.224, 0.225)),\n",
    "        ])\n",
    "        self.colors = self.generate_colors(21)\n",
    "\n",
    "    def generate_colors(self, num_classes):\n",
    "        random.seed(42)\n",
    "        return [tuple(random.choices(range(256), k=3)) for _ in range(num_classes)]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def segment(self, image: Image.Image):\n",
    "        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        output = self.model(input_tensor)[\"out\"]\n",
    "        mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "        return mask\n",
    "\n",
    "    def colorize_mask(self, mask):\n",
    "        color_mask = np.zeros((*mask.shape, 3), dtype=np.uint8)\n",
    "        for class_id, color in enumerate(self.colors):\n",
    "            color_mask[mask == class_id] = color\n",
    "        return Image.fromarray(color_mask)\n",
    "\n",
    "    def overlay(self, image, mask, alpha=0.6):\n",
    "        color_mask = self.colorize_mask(mask).convert(\"RGBA\")\n",
    "        base = image.convert(\"RGBA\").resize(color_mask.size)\n",
    "        blended = Image.blend(base, color_mask, alpha)\n",
    "        return blended\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2779346-ce15-44b1-b731-2c02839956ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class FeatureExtractor:\n",
    "    def __init__(self, device=None):\n",
    "        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        weights = ResNet50_Weights.DEFAULT\n",
    "        model = resnet50(weights=weights)\n",
    "        self.model = torch.nn.Sequential(*list(model.children())[:-1]).to(self.device).eval()\n",
    "        self.preprocess = weights.transforms()\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def __call__(self, image: Image.Image):\n",
    "        input_tensor = self.preprocess(image).unsqueeze(0).to(self.device)\n",
    "        output = self.model(input_tensor).squeeze().cpu()\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbcdec6-ccbd-4f7c-a5fa-a69dacc6d29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "def process_dataset(img_dir, out_dir=\"preproc\", captions_json=None, device=None):\n",
    "    img_dir = Path(img_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    seg = DeepLabSegmentation(device=device)\n",
    "    feats = FeatureExtractor(device=device)\n",
    "\n",
    "    caption_map = {}\n",
    "    if captions_json:\n",
    "        with open(captions_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        if isinstance(data, list):\n",
    "            for row in data:\n",
    "                caption_map.setdefault(row[\"image\"], []).append(row[\"caption\"])\n",
    "        elif isinstance(data, dict):\n",
    "            caption_map = {k: v if isinstance(v, list) else [v] for k, v in data.items()}\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    image_paths = [p for p in img_dir.rglob(\"*\") if p.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]]\n",
    "    print(f\"Found {len(image_paths)} images.\")\n",
    "\n",
    "    for img_path in tqdm(image_paths):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        mask = seg.segment(image)\n",
    "        mask_img = seg.colorize_mask(mask)\n",
    "        overlay_img = seg.overlay(image, mask)\n",
    "        feature_vector = feats(image)\n",
    "\n",
    "        stem = img_path.stem\n",
    "        mask_path = out_dir / f\"{stem}_mask.png\"\n",
    "        overlay_path = out_dir / f\"{stem}_overlay.jpg\"\n",
    "        feature_path = out_dir / f\"{stem}_feat.pt\"\n",
    "\n",
    "        mask_img.save(mask_path)\n",
    "        overlay_img.save(overlay_path)\n",
    "        torch.save(feature_vector, feature_path)\n",
    "\n",
    "        manifest.append({\n",
    "            \"image\": str(img_path),\n",
    "            \"mask\": str(mask_path.name),\n",
    "            \"overlay\": str(overlay_path.name),\n",
    "            \"features\": str(feature_path.name),\n",
    "            \"captions\": caption_map.get(img_path.name, [])\n",
    "        })\n",
    "\n",
    "    with open(out_dir / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in manifest:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… Done! Outputs saved in {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4340a2d-4d54-4a3f-a362-921f6f87bb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Imports\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "from pathlib import Path\n",
    "from typing import Union, List, Tuple, Optional\n",
    "import json, random, torch, numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Tiny wrappers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class DeepLabSeg:\n",
    "    \"\"\"Light wrapper around DeepLabV3-ResNet101 for fast reuse.\"\"\"\n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "        self.model = deeplabv3_resnet101(weights=weights).to(self.device).eval()\n",
    "        self.pre = weights.transforms()\n",
    "        random.seed(42)\n",
    "        self.palette = [tuple(random.choices(range(256), k=3)) for _ in range(21)]\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, img: Image.Image) -> np.ndarray:\n",
    "        t = self.pre(img).unsqueeze(0).to(self.device)\n",
    "        mask = self.model(t)[\"out\"].argmax(1).squeeze().cpu().numpy()\n",
    "        return mask.astype(np.uint8)\n",
    "\n",
    "    def colorize(self, mask: np.ndarray) -> Image.Image:\n",
    "        h, w = mask.shape\n",
    "        out = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        for cid, col in enumerate(self.palette):\n",
    "            out[mask == cid] = col\n",
    "        return Image.fromarray(out)\n",
    "\n",
    "    def overlay(self, img: Image.Image, mask: np.ndarray, alpha=0.6) -> Image.Image:\n",
    "        cm = self.colorize(mask).convert(\"RGBA\")\n",
    "        return Image.blend(img.convert(\"RGBA\").resize(cm.size), cm, alpha).convert(\"RGB\")\n",
    "\n",
    "\n",
    "class ResNetFeature:\n",
    "    \"\"\"2048-D global feature from ResNet-50 (avg-pooled).\"\"\"\n",
    "    def __init__(self, device: str = \"cpu\"):\n",
    "        self.device = device\n",
    "        w = ResNet50_Weights.DEFAULT\n",
    "        m = resnet50(weights=w)\n",
    "        self.model = torch.nn.Sequential(*list(m.children())[:-1]).to(self.device).eval()\n",
    "        self.pre = w.transforms()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, img: Image.Image) -> torch.Tensor:\n",
    "        t = self.pre(img).unsqueeze(0).to(self.device)\n",
    "        return self.model(t).squeeze().cpu()          # (2048,)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. The unified pipeline\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def run_pipeline(\n",
    "    images: Union[str, Path, List[Tuple[str, Image.Image]]],\n",
    "    out_dir: Union[str, Path] = \"preproc\",\n",
    "    captions: Optional[dict] = None,\n",
    "    device: str = \"cpu\",\n",
    "):\n",
    "    \"\"\"\n",
    "    â€¢ `images` can be\n",
    "        - a folder path\n",
    "        - a single image file path\n",
    "        - list of (name, PIL.Image) tuples (in-memory mode)\n",
    "    â€¢ `captions` optional dict  {name: [cap1, cap2, ...]}\n",
    "    \"\"\"\n",
    "    out_dir = Path(out_dir); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    seg   = DeepLabSeg(device)\n",
    "    feat  = ResNetFeature(device)\n",
    "    caps  = captions or {}\n",
    "    items = []\n",
    "\n",
    "    # -------- prepare iterable of (name, PIL.Image) --------\n",
    "    if isinstance(images, (str, Path)):\n",
    "        p = Path(images)\n",
    "        if p.is_dir():\n",
    "            for f in sorted(p.rglob(\"*\")):\n",
    "                if f.suffix.lower() in (\".jpg\", \".jpeg\", \".png\"):\n",
    "                    items.append((f.stem, Image.open(f).convert(\"RGB\")))\n",
    "        else:  # single file\n",
    "            items.append((p.stem, Image.open(p).convert(\"RGB\")))\n",
    "    else:  # already [(name, PIL.Image), ...]\n",
    "        items = images\n",
    "\n",
    "    print(f\"Processing {len(items)} image(s)â€¦\")\n",
    "    manifest = []\n",
    "    for name, img in tqdm(items):\n",
    "        msk = seg(img)\n",
    "        msk_img = seg.colorize(msk)\n",
    "        ovl_img = seg.overlay(img, msk)\n",
    "        vec = feat(img)\n",
    "\n",
    "        msk_path = out_dir / f\"{name}_mask.png\"\n",
    "        ovl_path = out_dir / f\"{name}_overlay.jpg\"\n",
    "        vec_path = out_dir / f\"{name}_feat.pt\"\n",
    "\n",
    "        msk_img.save(msk_path); ovl_img.save(ovl_path); torch.save(vec, vec_path)\n",
    "\n",
    "        manifest.append({\n",
    "            \"image\": name,          # simply a key; change if you prefer path\n",
    "            \"mask\":    msk_path.name,\n",
    "            \"overlay\": ovl_path.name,\n",
    "            \"feature\": vec_path.name,\n",
    "            \"captions\": caps.get(name, []),\n",
    "        })\n",
    "\n",
    "    with open(out_dir / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as fp:\n",
    "        fp.writelines(json.dumps(r)+\"\\n\" for r in manifest)\n",
    "\n",
    "    print(\"âœ… Done!  Outputs in â†’\", out_dir.resolve())\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Example calls\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# â€¢ Entire folder on CPU:\n",
    "# run_pipeline(\"images/\", out_dir=\"preproc_cpu\", device=\"cpu\")\n",
    "\n",
    "# â€¢ Just one file:\n",
    "# run_pipeline(\"images/cat.jpg\", out_dir=\"preproc_single\")\n",
    "\n",
    "# â€¢ In-memory mode:\n",
    "# imgs_mem = [(\"cat01\", img1), (\"dog02\", img2)]\n",
    "# run_pipeline(imgs_mem, out_dir=\"preproc_mem\", device=\"cpu\",\n",
    "#              captions={\"cat01\": [\"A cute cat.\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ecfa66-e032-4f95-bb00-4c75b311380d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, itertools, pprint, pathlib\n",
    "pp = pprint.PrettyPrinter(depth=2)\n",
    "\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "pp.pprint(rows[:3])          # quick peek\n",
    "print(\"Total samples:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8449480-4dd6-4af3-ae48-028c92ecaa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CaptionSet(Dataset):\n",
    "    def __init__(self, manifest, vocab):\n",
    "        self.manifest = manifest\n",
    "        self.vocab    = vocab       # word â†” id converter\n",
    "\n",
    "    def __len__(self): return len(self.manifest)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.manifest[idx]\n",
    "        # Load the 2048-D CNN feature vector\n",
    "        feat = torch.load(\"preproc/\" + row[\"feature\"])\n",
    "        # Pick ONE caption at random if multiple exist\n",
    "        caption = row[\"captions\"][0] if row[\"captions\"] else \"\"\n",
    "        # Numericalise caption â†’ tensor of token IDs\n",
    "        cap_ids = torch.tensor([self.vocab[\"<start>\"]] +\n",
    "                               [self.vocab.get(w, self.vocab[\"<unk>\"]) for w in caption.lower().split()] +\n",
    "                               [self.vocab[\"<end>\"]])\n",
    "        return feat, cap_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b53dc6-263e-4dd8-8c5e-8253be12420b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "# Load the manifest\n",
    "with open(\"preproc/manifest.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    rows = [json.loads(line) for line in f]\n",
    "\n",
    "# Safely collect all caption text (skipping None)\n",
    "all_captions = []\n",
    "for row in rows:\n",
    "    if \"captions\" in row and row[\"captions\"]:\n",
    "        all_captions.extend([cap for cap in row[\"captions\"] if cap])\n",
    "    elif \"caption\" in row and row[\"caption\"]:\n",
    "        all_captions.append(row[\"caption\"])\n",
    "\n",
    "# Build word frequency counter\n",
    "words = Counter(itertools.chain.from_iterable(cap.lower().split() for cap in all_captions))\n",
    "\n",
    "# Build vocab with special tokens and frequent words\n",
    "special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "itos = special + [w for w, n in words.items() if n >= 2]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "# Final vocab and size\n",
    "vocab = stoi\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d939a6c-ed19-4a7b-8a95-ab7eb1bf2a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, vocab, feat_dim=2048, emb_dim=512, hid_dim=512):\n",
    "        super().__init__()\n",
    "        vocab_size = len(vocab)\n",
    "        self.vocab = vocab\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb    = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab[\"<pad>\"])\n",
    "        self.lstm   = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        \"\"\"feats: (B,2048)  caps: (B,T)\"\"\"\n",
    "        h0 = torch.tanh(self.fc_img(feats)).unsqueeze(0)  # (1,B,H)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        emb = self.emb(caps)\n",
    "        out, _ = self.lstm(emb, (h0, c0))\n",
    "        logits = self.fc_out(out)  # (B,T,V)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63161b0-8106-4cd9-8927-9914f6521235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, max_len=20):\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                entry = json.loads(line)\n",
    "                for cap in entry[\"captions\"]:\n",
    "                    self.data.append((entry[\"features\"], cap))\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        words = caption.lower().split()\n",
    "        tokens = [\"<start>\"] + words[:self.max_len - 2] + [\"<end>\"]\n",
    "        ids = [self.vocab.get(w, self.vocab[\"<unk>\"]) for w in tokens]\n",
    "        pad_len = self.max_len - len(ids)\n",
    "        return torch.tensor(ids + [self.vocab[\"<pad>\"]] * pad_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_path, caption = self.data[idx]\n",
    "        feat = torch.load(feat_path)  # (2048,)\n",
    "        cap_tensor = self.encode_caption(caption)\n",
    "        return feat, cap_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecc9b9-0602-44e1-941d-184b673ce6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, itertools, torch\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 1: Ensure valid manifest\n",
    "# -------------------------------\n",
    "manifest_path = \"preproc/manifest.jsonl\"\n",
    "dummy_feat_path = \"dummy.pt\"\n",
    "\n",
    "# Create dummy data if manifest doesn't exist or is empty/invalid\n",
    "def ensure_valid_manifest():\n",
    "    valid_rows = []\n",
    "    if os.path.exists(manifest_path):\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    row = json.loads(line)\n",
    "                    if \"image\" in row and (\"captions\" in row or \"caption\" in row):\n",
    "                        captions = row.get(\"captions\") or [row.get(\"caption\")]\n",
    "                        if row[\"image\"] and any(captions):\n",
    "                            valid_rows.append(row)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "    if not valid_rows:\n",
    "        print(\"âš ï¸ Manifest invalid or empty. Creating dummy data.\")\n",
    "        os.makedirs(\"preproc\", exist_ok=True)\n",
    "        torch.save(torch.randn(2048), dummy_feat_path)\n",
    "        dummy_row = {\n",
    "            \"image\": dummy_feat_path,\n",
    "            \"captions\": [\"A dummy image showing a random scene.\"]\n",
    "        }\n",
    "        with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(dummy_row) + \"\\n\")\n",
    "        valid_rows = [dummy_row]\n",
    "\n",
    "    return valid_rows\n",
    "\n",
    "rows = ensure_valid_manifest()\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 2: Build vocabulary\n",
    "# -------------------------------\n",
    "all_captions = []\n",
    "for row in rows:\n",
    "    caps = row.get(\"captions\") or [row.get(\"caption\")]\n",
    "    all_captions.extend([cap for cap in caps if cap])\n",
    "\n",
    "words = Counter(itertools.chain.from_iterable(cap.lower().split() for cap in all_captions))\n",
    "special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "itos = special + [w for w, n in words.items() if n >= 1]\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "vocab = stoi\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"âœ… Vocabulary built with {vocab_size} tokens\")\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 3: CaptionDataset\n",
    "# -------------------------------\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, max_len=20):\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    row = json.loads(line)\n",
    "                    image_path = row.get(\"image\")\n",
    "                    captions = row.get(\"captions\") or [row.get(\"caption\")]\n",
    "                    if not image_path or not captions:\n",
    "                        continue\n",
    "                    for cap in captions:\n",
    "                        if cap:\n",
    "                            self.data.append((image_path, cap))\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "        self.max_len = max_len\n",
    "        print(f\"ğŸ“¦ Loaded {len(self.data)} usable image-caption pairs\")\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        tokens = caption.lower().split()\n",
    "        ids = [self.vocab[\"<start>\"]] + [self.vocab.get(t, self.vocab[\"<unk>\"]) for t in tokens] + [self.vocab[\"<end>\"]]\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [self.vocab[\"<pad>\"]] * (self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_path, caption = self.data[idx]\n",
    "        feat = torch.load(feat_path)\n",
    "        cap_tensor = self.encode_caption(caption)\n",
    "        return feat, cap_tensor\n",
    "\n",
    "# -------------------------------\n",
    "# STEP 4: DataLoader\n",
    "# -------------------------------\n",
    "train_ds = CaptionDataset(manifest_path, vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787a737f-5120-4cf8-b4c5-9f3da2496919",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "image_dir = Path(\"images/\")\n",
    "image_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "print(f\"Found {len(image_files)} image(s).\")\n",
    "for img in image_files[:5]:\n",
    "    print(\"-\", img.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa197ca-b77e-4b19-82a0-b45aac5b341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def process_dataset(img_dir, out_dir, captions_json=None, device=\"cpu\"):\n",
    "    img_dir = Path(img_dir)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Load captions if provided\n",
    "    caption_map = {}\n",
    "    if captions_json:\n",
    "        with open(captions_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            caption_map = json.load(f)\n",
    "\n",
    "    # Load DeepLab model and preprocessing\n",
    "    weights = deeplabv3_resnet50(pretrained=True).eval()\n",
    "    model = weights.to(device)\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    # Color palette for segmentation\n",
    "    palette = weights.meta[\"palette\"]\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    for img_path in tqdm(sorted(img_dir.glob(\"*\"))):\n",
    "        if not img_path.suffix.lower() in [\".jpg\", \".jpeg\", \".png\"]: continue\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")  # force RGB\n",
    "            input_tensor = preprocess(img).unsqueeze(0).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = model(input_tensor)[\"out\"][0].argmax(0).cpu().numpy()\n",
    "\n",
    "            # Color mask\n",
    "            color_mask = np.zeros((output.shape[0], output.shape[1], 3), dtype=np.uint8)\n",
    "            for label, color in enumerate(palette):\n",
    "                color_mask[output == label] = color\n",
    "\n",
    "            mask_img = Image.fromarray(color_mask)\n",
    "            overlay = Image.blend(img.resize(mask_img.size), mask_img, alpha=0.5)\n",
    "\n",
    "            # Simulate feature vector (real project: use precomputed CNN features)\n",
    "            feature_vector = torch.randn(2048)  # Dummy\n",
    "\n",
    "            # File naming\n",
    "            stem = img_path.stem\n",
    "            mask_path = out_dir / f\"{stem}_mask.png\"\n",
    "            overlay_path = out_dir / f\"{stem}_overlay.jpg\"\n",
    "            feature_path = out_dir / f\"{stem}_feat.pt\"\n",
    "\n",
    "            # âœ… Fix RGBA to RGB conversion before saving JPEG\n",
    "            overlay.convert(\"RGB\").save(overlay_path)\n",
    "            mask_img.save(mask_path)\n",
    "            torch.save(feature_vector, feature_path)\n",
    "\n",
    "            manifest.append({\n",
    "                \"image\": str(feature_path),\n",
    "                \"mask\": str(mask_path.name),\n",
    "                \"overlay\": str(overlay_path.name),\n",
    "                \"captions\": caption_map.get(img_path.name, [])\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Skipped {img_path.name}: {e}\")\n",
    "\n",
    "    # Save final manifest\n",
    "    with open(out_dir / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in manifest:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… Processed {len(manifest)} images and saved to {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6b7b08-98b8-4f99-b967-c4d8df1f8c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "print(\"âœ… Total samples:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74821fe9-1ceb-4e57-9d23-d35048ae15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    img_dir=\".\",              # âœ… This points to the current folder where your .jpg is\n",
    "    out_dir=\"preproc/\",\n",
    "    captions_json=None,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401456df-bbe0-4e66-88f6-826146aa98c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pathlib\n",
    "\n",
    "manifest_path = pathlib.Path(\"preproc/manifest.jsonl\")\n",
    "rows = [json.loads(line) for line in manifest_path.open()]\n",
    "print(\"âœ… Total usable samples:\", len(rows))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfefee7a-95b5-4d4c-b3e4-2a8152abdfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "img_path = \"photo_2025-06-09_21-10-49.jpg\"\n",
    "try:\n",
    "    img = Image.open(img_path)\n",
    "    img.verify()\n",
    "    print(\"âœ… Image is valid and readable.\")\n",
    "except Exception as e:\n",
    "    print(\"âŒ Error loading image:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0859f23-ced9-4937-9bcc-b131e45c1b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# âœ… Define image directory and collect image paths\n",
    "image_dir = Path(\"images/\")\n",
    "img_files = list(image_dir.glob(\"*.jpg\")) + list(image_dir.glob(\"*.jpeg\")) + list(image_dir.glob(\"*.png\"))\n",
    "\n",
    "print(f\"ğŸ“¸ Total images found: {len(img_files)}\")\n",
    "\n",
    "# âœ… Try to open each image safely\n",
    "for img_path in tqdm(img_files):\n",
    "    print(\"ğŸ–¼ï¸ Found:\", img_path)\n",
    "    try:\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        print(\"âœ… Image is valid and readable.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to open {img_path}: {e}\")\n",
    "        continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc7e03-d5f7-4711-ac00-ecae72582291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "def process_dataset(img_dir, out_dir, captions_json=None, device=\"cpu\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "\n",
    "    model = resnet50(pretrained=True)\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Get list of images\n",
    "    supported_exts = (\".jpg\", \".jpeg\", \".png\")\n",
    "    img_files = [os.path.join(img_dir, f) for f in os.listdir(img_dir) if f.lower().endswith(supported_exts)]\n",
    "\n",
    "    print(f\"ğŸ” Found {len(img_files)} images in {img_dir}\")\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    for img_path in tqdm(img_files):\n",
    "        print(\"ğŸ–¼ï¸ Checking:\", img_path)\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error reading {img_path}: {e}\")\n",
    "            continue\n",
    "\n",
    "        image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            features = model(image_tensor).squeeze().cpu().numpy().tolist()\n",
    "\n",
    "        sample = {\n",
    "            \"image\": os.path.abspath(img_path),\n",
    "            \"features\": features,\n",
    "            \"caption\": None  # Will be filled later if captions_json provided\n",
    "        }\n",
    "        manifest.append(sample)\n",
    "\n",
    "    print(f\"âœ… Total usable samples: {len(manifest)}\")\n",
    "\n",
    "    # Save manifest\n",
    "    with open(os.path.join(out_dir, \"manifest.jsonl\"), \"w\", encoding=\"utf-8\") as f:\n",
    "        for row in manifest:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "    print(f\"ğŸ“¦ Manifest saved to {out_dir}/manifest.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254dae5a-b658-4bff-a3c0-48ec1223cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_dataset(\n",
    "    img_dir=\".\",              # since your image is in current folder\n",
    "    out_dir=\"preproc/\",\n",
    "    captions_json=None,\n",
    "    device=\"cpu\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837718c8-4e82-4bf8-bb8e-759bad4e5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Helper tokenizer\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "# Build vocab from dummy captions or predefined words\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold=1):\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "    \n",
    "    def build_vocab(self, sentence_list):\n",
    "        frequencies = Counter()\n",
    "        idx = 4\n",
    "\n",
    "        for sentence in sentence_list:\n",
    "            tokens = tokenize(sentence)\n",
    "            frequencies.update(tokens)\n",
    "\n",
    "        for word, freq in frequencies.items():\n",
    "            if freq >= self.freq_threshold:\n",
    "                self.stoi[word] = idx\n",
    "                self.itos[idx] = word\n",
    "                idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = tokenize(text)\n",
    "        return [\n",
    "            self.stoi.get(token, self.stoi[\"<UNK>\"])\n",
    "            for token in tokenized_text\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3b93b8-f86a-486b-9465-62ef74cca29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab):\n",
    "        with open(manifest_path, 'r') as f:\n",
    "            self.samples = [json.loads(line) for line in f]\n",
    "        self.vocab = vocab\n",
    "        self.max_len = 20  # cap caption length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        caption = sample.get(\"caption\") or \"a sample image\"\n",
    "        caption_idxs = [self.vocab.stoi[\"<SOS>\"]] + self.vocab.numericalize(caption)[:self.max_len] + [self.vocab.stoi[\"<EOS>\"]]\n",
    "        padded = caption_idxs + [self.vocab.stoi[\"<PAD>\"]] * (self.max_len + 2 - len(caption_idxs))\n",
    "        return torch.tensor(sample[\"features\"], dtype=torch.float32), torch.tensor(padded, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1010ee-d21a-4cd0-911d-fda9c61709dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torchvision.transforms as T\n",
    "from torchvision import models\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# --------------------------\n",
    "# 1. Process Dataset: Save features and manifest\n",
    "# --------------------------\n",
    "def process_dataset(img_dir=\"images/\", out_dir=\"preproc/\", captions_json=None, device=\"cpu\"):\n",
    "    device = torch.device(device)\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    model = models.resnet50(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model = model.to(device).eval()\n",
    "\n",
    "    preprocess = T.Compose([\n",
    "        T.Resize((224, 224)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    img_dir = Path(img_dir)\n",
    "    image_files = list(img_dir.glob(\"*.jpg\")) + list(img_dir.glob(\"*.jpeg\")) + list(img_dir.glob(\"*.png\"))\n",
    "\n",
    "    caption_map = {}\n",
    "    if captions_json and os.path.exists(captions_json):\n",
    "        with open(captions_json, \"r\", encoding=\"utf-8\") as f:\n",
    "            caption_map = json.load(f)\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    for img_path in tqdm(image_files):\n",
    "        try:\n",
    "            image = Image.open(img_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Cannot open {img_path.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            img_tensor = preprocess(image).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                features = model(img_tensor).squeeze().cpu()\n",
    "\n",
    "            stem = img_path.stem\n",
    "            feat_path = out_dir / f\"{stem}_feat.pt\"\n",
    "            torch.save(features, feat_path)\n",
    "\n",
    "            manifest.append({\n",
    "                \"features\": str(feat_path),\n",
    "                \"captions\": caption_map.get(img_path.name, [\"a sample image\"])\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Failed processing {img_path.name}: {e}\")\n",
    "\n",
    "    with open(out_dir / \"manifest.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for item in manifest:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "    print(f\"âœ… Processed {len(manifest)} images into features and manifest.\")\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 2. Vocabulary class\n",
    "# --------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.itos = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        idx = len(self.stoi)\n",
    "        for sentence in captions:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos.append(word)\n",
    "                    idx += 1\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        return [self.stoi.get(word, self.stoi[\"<unk>\"]) for word in sentence.lower().split()]\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 3. Caption Dataset class\n",
    "# --------------------------\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, max_len=20):\n",
    "        self.vocab = vocab\n",
    "        self.data = []\n",
    "        self.max_len = max_len\n",
    "\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    entry = json.loads(line)\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "                feat_path = entry.get(\"features\")\n",
    "                if not feat_path or not os.path.exists(feat_path): continue\n",
    "\n",
    "                caps = entry.get(\"captions\") or [entry.get(\"caption\")]\n",
    "                if not caps or caps[0] is None: continue\n",
    "\n",
    "                for cap in caps:\n",
    "                    if cap:\n",
    "                        self.data.append((feat_path, cap))\n",
    "\n",
    "        print(f\"âœ… Loaded {len(self.data)} valid samples.\")\n",
    "\n",
    "    def encode_caption(self, caption):\n",
    "        tokens = self.vocab.encode(caption)\n",
    "        tokens = tokens[:self.max_len]\n",
    "        return torch.tensor(tokens + [0] * (self.max_len - len(tokens)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_path, caption = self.data[idx]\n",
    "        feat = torch.load(feat_path)\n",
    "        cap_tensor = self.encode_caption(caption)\n",
    "        return feat, cap_tensor\n",
    "\n",
    "\n",
    "# --------------------------\n",
    "# 4. Run Everything\n",
    "# --------------------------\n",
    "# Step 1: Process dataset (only once)\n",
    "process_dataset(\n",
    "    img_dir=\"images/\",\n",
    "    out_dir=\"preproc/\",\n",
    "    captions_json=None,\n",
    "    device=\"cpu\"  # or \"cuda\"\n",
    ")\n",
    "\n",
    "# Step 2: Dummy captions to build vocab\n",
    "dummy_captions = [\"a segmented image\", \"an example picture\"]\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(dummy_captions)\n",
    "\n",
    "# Step 3: Build dataset and dataloader\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab)\n",
    "if len(train_ds) == 0:\n",
    "    raise ValueError(\"âŒ Dataset is empty. Please check image folder or feature generation.\")\n",
    "else:\n",
    "    train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "    print(f\"ğŸ“¦ DataLoader ready with {len(train_ds)} samples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b75395-3aff-4bb9-bdf8-92c248e78c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import json\n",
    "import os\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, transform=None):\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            self.samples = [json.loads(line) for line in f]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        img = Image.open(sample['image']).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # Use dummy caption if not available\n",
    "        caption = sample.get('caption', \"a photo\")\n",
    "        tokens = self.vocab.tokenize(caption)\n",
    "        caption_ids = [self.vocab.start_token_id] + self.vocab.encode(tokens) + [self.vocab.end_token_id]\n",
    "\n",
    "        return img, torch.tensor(caption_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30eee98-7ebf-4d0c-aac8-821f00ecee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # Encode image\n",
    "        img_feats = self.fc_img(img_feats).unsqueeze(1)  # (B, 1, H)\n",
    "        embeds = self.embedding(captions)  # (B, T, E)\n",
    "        x = torch.cat([img_feats, embeds], dim=1)  # (B, T+1, E)\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6006cb-3d36-4b09-a8f7-010d5330a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    \"\"\"Collate function to pad caption sequences in a batch.\"\"\"\n",
    "    images, captions = zip(*batch)\n",
    "    \n",
    "    # Stack images (they're already tensors)\n",
    "    images = torch.stack(images, dim=0)\n",
    "\n",
    "    # Pad captions\n",
    "    captions = pad_sequence(captions, batch_first=True, padding_value=0)  # 0 = <PAD> token\n",
    "\n",
    "    return images, captions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11d4afb-6482-4738-84a8-79a9e1e522e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609012e-2ed1-4e8c-ab8f-c002d6ef5f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=None):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.rnn = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        # img_feat: (B, feat_dim)\n",
    "        h0 = self.fc_img(img_feat).unsqueeze(0)  # (1, B, H)\n",
    "        x = self.embed(captions)                 # (B, T, E)\n",
    "        out, _ = self.rnn(x, h0)                 # (B, T, H)\n",
    "        return self.fc_out(out)                  # (B, T, vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c90c29c-efef-4b8a-9e8f-772995e0c846",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# âœ… Fix the vocab size and pad index\n",
    "vocab_size = len(vocab.stoi)\n",
    "pad_idx = vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# âœ… Create the model with correct parameters\n",
    "model = Captioner(vocab_size=vocab_size, pad_idx=pad_idx).to(device)\n",
    "\n",
    "# âœ… Define optimizer and loss using correct pad index\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11ebcb5-fcd4-40c7-a863-8d218df7cb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab.itos)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cff3766-7e8a-45fb-91c5-7c9e76251909",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Captioner(vocab_size=len(vocab.itos)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d094ee7-30d4-4524-94c7-e0e2d1339c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Correctly access vocabulary size and <pad> index\n",
    "vocab_size = len(vocab.itos)\n",
    "pad_idx = vocab.stoi[\"<pad>\"]\n",
    "\n",
    "# Initialize model, optimizer, and loss\n",
    "model = Captioner(vocab_size=vocab_size, pad_idx=pad_idx).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad90d5e-34e7-4e16-9532-c106a553adbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Vocabulary Class ---\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.itos = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.itos)}\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        idx = len(self.stoi)\n",
    "        for sent in captions:\n",
    "            for word in sent.lower().split():\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos.append(word)\n",
    "                    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccdba0d-ef09-4e09-b869-e6d8997d453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.special_tokens = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.itos = list(self.special_tokens)\n",
    "        self.stoi = {token: idx for idx, token in enumerate(self.itos)}\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        idx = len(self.stoi)\n",
    "        for sentence in captions:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos.append(word)\n",
    "                    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6136067-aa73-4770-9e64-b77af97d0c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. Imports\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, re, random, requests, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Setup paths and download image\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_DIR = Path(\"images\")\n",
    "FEATURE_DIR = Path(\"preproc\")\n",
    "MANIFEST = FEATURE_DIR / \"manifest.jsonl\"\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FEATURE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "if not any(IMAGE_DIR.glob(\"*.jpg\")):\n",
    "    print(\"ğŸ“¥ Downloading demo image...\")\n",
    "    url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/640px-Cat03.jpg\"\n",
    "    img_data = requests.get(url).content\n",
    "    with open(IMAGE_DIR / \"demo.jpg\", \"wb\") as f:\n",
    "        f.write(img_data)\n",
    "    print(\"âœ… Demo image saved\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Feature extraction and manifest creation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_dataset(image_dir, manifest_path, feature_dir):\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    manifest = []\n",
    "\n",
    "    for img_path in image_dir.glob(\"*.jpg\"):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        tensor = transform(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            feat = model(tensor).squeeze().clone()\n",
    "        feat_path = feature_dir / f\"{img_path.stem}.pt\"\n",
    "        torch.save(feat, feat_path)\n",
    "        manifest.append({\n",
    "            \"feature_path\": str(feat_path),\n",
    "            \"captions\": [\"a photo of a cat\"]\n",
    "        })\n",
    "\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        for entry in manifest:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    print(f\"âœ… Manifest created with {len(manifest)} entries\")\n",
    "\n",
    "if not MANIFEST.exists() or os.stat(MANIFEST).st_size == 0:\n",
    "    process_dataset(IMAGE_DIR, MANIFEST, FEATURE_DIR)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Vocabulary\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.itos = self.special.copy()\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.special)}\n",
    "\n",
    "    def build(self, captions):\n",
    "        words = re.findall(r\"\\w+\", \" \".join(captions).lower())\n",
    "        for word in sorted(set(words)):\n",
    "            if word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = re.findall(r\"\\w+\", text.lower())\n",
    "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Load manifest + check\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rows = []\n",
    "with open(MANIFEST) as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line.strip())\n",
    "        path = Path(data.get(\"feature_path\") or data.get(\"feature\", \"\"))\n",
    "        if path.exists():\n",
    "            data[\"feature_path\"] = str(path)\n",
    "            rows.append(data)\n",
    "\n",
    "if not rows:\n",
    "    print(\"âŒ No valid feature paths found. Regenerating...\")\n",
    "    process_dataset(IMAGE_DIR, MANIFEST, FEATURE_DIR)\n",
    "    with open(MANIFEST) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            path = Path(data.get(\"feature_path\") or data.get(\"feature\", \"\"))\n",
    "            if path.exists():\n",
    "                data[\"feature_path\"] = str(path)\n",
    "                rows.append(data)\n",
    "\n",
    "if not rows:\n",
    "    raise RuntimeError(\"âŒ Still no valid data rows found. Please check image/feature paths.\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Build vocab\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "vocab = Vocabulary()\n",
    "all_captions = [cap for row in rows for cap in row[\"captions\"]]\n",
    "vocab.build(all_captions)\n",
    "print(\"âœ… Vocab size:\", len(vocab))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. Dataset + Dataloader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, rows, vocab, max_len=20):\n",
    "        self.rows = rows\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        feat = torch.load(row[\"feature_path\"])\n",
    "        cap = random.choice(row[\"captions\"])\n",
    "        ids = [self.vocab.stoi[\"<start>\"]] + self.vocab.encode(cap)[:self.max_len] + [self.vocab.stoi[\"<end>\"]]\n",
    "        return feat, torch.tensor(ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats, caps = zip(*batch)\n",
    "    feats = torch.stack(feats)\n",
    "    caps = pad_sequence(caps, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n",
    "    return feats, caps\n",
    "\n",
    "train_ds = CaptionDataset(rows, vocab)\n",
    "if len(train_ds) == 0:\n",
    "    raise ValueError(\"âŒ No samples in dataset.\")\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. Captioner model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.img_fc = nn.Linear(2048, 512)\n",
    "        self.emb = nn.Embedding(len(vocab), 256, padding_idx=vocab.stoi[\"<pad>\"])\n",
    "        self.gru = nn.GRU(256, 512, batch_first=True)\n",
    "        self.fc_out = nn.Linear(512, len(vocab))\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h0 = torch.tanh(self.img_fc(feats)).unsqueeze(0)\n",
    "        x = self.emb(caps)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 8. Training loop (âœ… FIXED LINE BELOW)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Captioner().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])  # âœ… FIXED\n",
    "\n",
    "print(\"ğŸš€ Starting training...\")\n",
    "for epoch in range(5):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for feats, caps in train_dl:\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(feats, caps[:, :-1])\n",
    "        loss = criterion(output.reshape(-1, output.size(-1)), caps[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"ğŸ“˜ Epoch {epoch+1} â€” Loss: {total_loss / len(train_dl):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728c142-cfed-4b27-bc79-0e17b8a8e59b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Paths\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "manifest_path = Path(\"preproc/manifest.jsonl\")\n",
    "image_root = Path(\"images\")  # directory containing original .jpg images\n",
    "out_dir = Path(\"preproc/features\")  # directory to save .pt feature files\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Load pre-trained ResNet model for feature extraction\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])  # remove final classification layer\n",
    "model.eval()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Image preprocessing\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Load and validate manifest\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "rows = []\n",
    "with open(manifest_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        row = json.loads(line.strip())\n",
    "        if \"image\" in row:\n",
    "            rows.append(row)\n",
    "        else:\n",
    "            print(\"âš ï¸ Skipping row without 'image' key:\", row)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Process images, extract features, and update manifest\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "for row in tqdm(rows, desc=\"Extracting features\"):\n",
    "    try:\n",
    "        img_name = row[\"image\"]\n",
    "        img_path = image_root / img_name\n",
    "\n",
    "        if not img_path.exists():\n",
    "            raise FileNotFoundError(f\"Image file not found: {img_path}\")\n",
    "\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0)  # shape: (1, 3, 224, 224)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = model(x).squeeze()  # shape: (2048,)\n",
    "\n",
    "        # Save feature vector\n",
    "        feat_file = f\"{img_path.stem}.pt\"\n",
    "        feat_path = out_dir / feat_file\n",
    "        torch.save(feat, feat_path)\n",
    "\n",
    "        # Update row with new 'feature' key\n",
    "        row[\"feature\"] = str(Path(\"features\") / feat_file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error processing image: {row.get('image', 'UNKNOWN')} â€” {e}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Save updated manifest\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "updated_manifest_path = Path(\"preproc/manifest.jsonl\")\n",
    "with open(updated_manifest_path, \"w\") as f:\n",
    "    for row in rows:\n",
    "        if \"feature\" in row:  # only keep rows with valid features\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "\n",
    "print(f\"âœ… Updated manifest saved to: {updated_manifest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9bb5f4-8333-4497-95d5-2e4f634af24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, torch, random\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 1: Setup paths\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_DIR = Path(\"images\")\n",
    "FEATURE_DIR = Path(\"preproc/features\")\n",
    "MANIFEST = Path(\"preproc/manifest.jsonl\")\n",
    "\n",
    "IMAGE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 2: Download demo image if missing\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "demo_img_path = IMAGE_DIR / \"demo.jpg\"\n",
    "if not demo_img_path.exists():\n",
    "    print(\"ğŸ“¥ Downloading demo image...\")\n",
    "    import requests\n",
    "    url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/640px-Cat03.jpg\"\n",
    "    img_data = requests.get(url).content\n",
    "    with open(demo_img_path, \"wb\") as f:\n",
    "        f.write(img_data)\n",
    "    print(\"âœ… Demo image saved\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 3: Extract features & create manifest\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_image_and_create_manifest():\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    rows = []\n",
    "    for img_path in IMAGE_DIR.glob(\"*.jpg\"):\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        x = transform(img).unsqueeze(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            feat = model(x).squeeze()\n",
    "\n",
    "        feat_path = FEATURE_DIR / f\"{img_path.stem}.pt\"\n",
    "        torch.save(feat, feat_path)\n",
    "\n",
    "        rows.append({\n",
    "            \"image\": img_path.name,\n",
    "            \"feature\": f\"features/{img_path.stem}.pt\",\n",
    "            \"captions\": [\"a photo of a cat\"]\n",
    "        })\n",
    "\n",
    "    # Save manifest\n",
    "    with open(MANIFEST, \"w\") as f:\n",
    "        for row in rows:\n",
    "            f.write(json.dumps(row) + \"\\n\")\n",
    "    print(f\"âœ… Manifest created with {len(rows)} entries\")\n",
    "\n",
    "# Only run if manifest missing or empty\n",
    "if not MANIFEST.exists() or MANIFEST.stat().st_size == 0:\n",
    "    process_image_and_create_manifest()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 4: Load manifest safely\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def load_manifest():\n",
    "    rows = []\n",
    "    with open(MANIFEST, \"r\") as f:\n",
    "        for line in f:\n",
    "            row = json.loads(line.strip())\n",
    "            feature_file = row.get(\"feature\")\n",
    "            if feature_file and (Path(\"preproc\") / feature_file).exists():\n",
    "                row[\"feature\"] = str(Path(\"preproc\") / feature_file)\n",
    "                rows.append(row)\n",
    "            else:\n",
    "                print(f\"âš ï¸ Missing feature file for: {row.get('image')}\")\n",
    "    if not rows:\n",
    "        raise ValueError(\"âŒ No valid entries in manifest. Check your paths and feature files.\")\n",
    "    return rows\n",
    "\n",
    "rows = load_manifest()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 5: Dummy Vocabulary\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class DummyVocab:\n",
    "    def __init__(self):\n",
    "        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "    def encode(self, text): return [4, 5, 6]\n",
    "vocab = DummyVocab()\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Step 6: Dataset + Dataloader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, rows, vocab, max_len=20):\n",
    "        self.rows = rows\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        feat = torch.load(row[\"feature\"])\n",
    "        cap = random.choice(row[\"captions\"])\n",
    "        ids = [self.vocab.stoi[\"<start>\"]] + self.vocab.encode(cap)[:self.max_len] + [self.vocab.stoi[\"<end>\"]]\n",
    "        return feat, torch.tensor(ids)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats, caps = zip(*batch)\n",
    "    feats = torch.stack(feats)\n",
    "    caps = pad_sequence(caps, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n",
    "    return feats, caps\n",
    "\n",
    "train_ds = CaptionDataset(rows, vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"âœ… Dataset ready with {len(train_ds)} samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9003d2d-fbcb-4516-8a63-b84f37ab40b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Dummy vocab for demonstration\n",
    "class DummyVocab:\n",
    "    def __init__(self):\n",
    "        self.itos = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\", \"cat\", \"photo\"]\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.itos)}\n",
    "vocab = DummyVocab()\n",
    "\n",
    "# âœ… Corrected Captioner model\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, vocab_size, pad_idx):\n",
    "        super().__init__()\n",
    "        self.img_fc = nn.Linear(2048, 512)\n",
    "        self.emb = nn.Embedding(vocab_size, 256, padding_idx=pad_idx)\n",
    "        self.gru = nn.GRU(256, 512, batch_first=True)\n",
    "        self.fc_out = nn.Linear(512, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h0 = torch.tanh(self.img_fc(feats)).unsqueeze(0)\n",
    "        x = self.emb(caps)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# âœ… Instantiate with correct args\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Captioner(vocab_size=len(vocab.itos), pad_idx=vocab.stoi[\"<pad>\"]).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "\n",
    "print(\"âœ… Model initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69cc21b2-3a49-4a37-bb52-933dbbfaaaac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------------------------\n",
    "# Vocabulary Class\n",
    "# --------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.itos = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        idx = len(self.stoi)\n",
    "        for sentence in captions:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos.append(word)\n",
    "                    idx += 1\n",
    "\n",
    "    def encode(self, sentence, max_len=20):\n",
    "        tokens = self.stoi.get(\"<start>\") and [self.stoi[\"<start>\"]]\n",
    "        tokens += [self.stoi.get(word, self.stoi[\"<unk>\"]) for word in sentence.lower().split()]\n",
    "        tokens.append(self.stoi[\"<end>\"])\n",
    "        tokens = tokens[:max_len]\n",
    "        return torch.tensor(tokens + [self.stoi[\"<pad>\"]] * (max_len - len(tokens)))\n",
    "\n",
    "# --------------------------\n",
    "# CaptionDataset Class\n",
    "# --------------------------\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, max_len=20):\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        self.rows = []\n",
    "\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                row = json.loads(line)\n",
    "                if \"features\" in row and os.path.exists(row[\"features\"]):\n",
    "                    if \"captions\" in row and row[\"captions\"]:\n",
    "                        self.rows.append(row)\n",
    "\n",
    "        print(f\"âœ… Loaded {len(self.rows)} valid samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        feat = torch.load(row[\"features\"])  # already 2048-D feature\n",
    "        caption = random.choice(row[\"captions\"])\n",
    "        cap_tensor = self.vocab.encode(caption, self.max_len)\n",
    "        return feat, cap_tensor\n",
    "\n",
    "# --------------------------\n",
    "# Simple Captioning Model\n",
    "# --------------------------\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=256, vocab_size=1000, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb    = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm   = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h0 = torch.tanh(self.fc_img(feats)).unsqueeze(0)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        emb = self.emb(caps)\n",
    "        out, _ = self.lstm(emb, (h0, c0))\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# --------------------------\n",
    "# Build Vocab & Load Data\n",
    "# --------------------------\n",
    "dummy_captions = [\"a segmented image\", \"an example picture\"]\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(dummy_captions)\n",
    "vocab_size = len(vocab.stoi)\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "# --------------------------\n",
    "# Train Model\n",
    "# --------------------------\n",
    "model = Captioner(vocab_size=vocab_size, pad_idx=vocab.stoi[\"<pad>\"]).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for feats, caps in train_dl:\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(feats, caps[:, :-1])  # predict next token\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), caps[:, 1:].reshape(-1))  # ignore <start>\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"ğŸ“˜ Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a72499-30de-4fb4-8eb3-8b4801c5d3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, jsonl_path, vocab, transform=None):\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        # âœ… Properly load and parse each line as a JSON object (dict)\n",
    "        with open(jsonl_path, 'r') as f:\n",
    "            self.rows = [json.loads(line) for line in f]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        \n",
    "        # âœ… Load precomputed image feature\n",
    "        feat_path = Path(\"preproc\") / row[\"feature\"]\n",
    "        feat = torch.load(feat_path)  # should be shape (2048,)\n",
    "        \n",
    "        # âœ… Pick one caption randomly\n",
    "        caption = random.choice(row[\"captions\"]).lower()\n",
    "        \n",
    "        # âœ… Tokenize and convert to indices\n",
    "        tokens = [\"<start>\"] + caption.split() + [\"<end>\"]\n",
    "        token_ids = [self.vocab.stoi.get(token, self.vocab.stoi[\"<unk>\"]) for token in tokens]\n",
    "        cap_tensor = torch.tensor(token_ids, dtype=torch.long)\n",
    "\n",
    "        return feat, cap_tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e94c1-4686-4cb4-8740-9900a4158e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab, transform)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186859bb-8012-49cb-ac1d-7c8b68410413",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Captioner model definition\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        img_feat = self.fc_img(img_feat).unsqueeze(1)  # (B, 1, H)\n",
    "        cap_embeds = self.embed(captions)              # (B, T, E)\n",
    "        x = torch.cat([img_feat, cap_embeds[:, :-1, :]], dim=1)  # Shifted input\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model, optimizer, loss\n",
    "vocab_size = len(vocab.itos)\n",
    "model = Captioner(vocab_size=vocab_size).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "\n",
    "print(f\"âœ… Model ready on {device} with vocab size {vocab_size}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a07223-6442-44a7-9981-1d8f4e736d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json, random, os\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------\n",
    "# Vocabulary Class\n",
    "# --------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.stoi = {\"<pad>\": 0, \"<start>\": 1, \"<end>\": 2, \"<unk>\": 3}\n",
    "        self.itos = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "\n",
    "    def build_vocab(self, captions):\n",
    "        idx = len(self.stoi)\n",
    "        for sentence in captions:\n",
    "            for word in sentence.lower().split():\n",
    "                if word not in self.stoi:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos.append(word)\n",
    "                    idx += 1\n",
    "\n",
    "    def encode(self, sentence, max_len=20):\n",
    "        tokens = [self.stoi[\"<start>\"]]\n",
    "        tokens += [self.stoi.get(word, self.stoi[\"<unk>\"]) for word in sentence.lower().split()]\n",
    "        tokens.append(self.stoi[\"<end>\"])\n",
    "        tokens = tokens[:max_len]\n",
    "        return torch.tensor(tokens + [self.stoi[\"<pad>\"]] * (max_len - len(tokens)))\n",
    "\n",
    "# --------------------------\n",
    "# Dataset Class\n",
    "# --------------------------\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_path, vocab, max_len=20):\n",
    "        self.data = []\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "        with open(manifest_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                row = json.loads(line)\n",
    "                if \"features\" in row and os.path.exists(row[\"features\"]):\n",
    "                    if \"captions\" in row and row[\"captions\"]:\n",
    "                        self.data.append(row)\n",
    "        print(f\"âœ… Loaded {len(self.data)} samples.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data[idx]\n",
    "        feat = torch.load(row[\"features\"])  # (2048,)\n",
    "        caption = random.choice(row[\"captions\"])\n",
    "        cap_tensor = self.vocab.encode(caption, self.max_len)\n",
    "        return feat, cap_tensor\n",
    "\n",
    "# --------------------------\n",
    "# Captioner Model\n",
    "# --------------------------\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=256, vocab_size=1000, pad_idx=0):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h0 = torch.tanh(self.fc_img(feats)).unsqueeze(0)\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        emb = self.emb(caps)\n",
    "        out, _ = self.lstm(emb, (h0, c0))\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# --------------------------\n",
    "# Build vocab, dataset and dataloader\n",
    "# --------------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dummy_captions = [\"a segmented image\", \"an example picture\"]\n",
    "vocab = Vocabulary()\n",
    "vocab.build_vocab(dummy_captions)\n",
    "PAD_IDX = vocab.stoi[\"<pad>\"]  # âœ… Define PAD_IDX here\n",
    "vocab_size = len(vocab.stoi)\n",
    "\n",
    "train_ds = CaptionDataset(\"preproc/manifest.jsonl\", vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=2, shuffle=True)\n",
    "\n",
    "# --------------------------\n",
    "# Initialize model, loss and optimizer\n",
    "# --------------------------\n",
    "model = Captioner(vocab_size=vocab_size, pad_idx=PAD_IDX).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)  # âœ… Now PAD_IDX is defined\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# --------------------------\n",
    "# Training Loop\n",
    "# --------------------------\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for feats, caps in train_dl:\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(feats, caps[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, vocab_size), caps[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"ğŸ“˜ Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ac60cf-8790-4a4f-8a90-bc4637574d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=512, hid_dim=512, vocab_size=1000):\n",
    "        super().__init__()\n",
    "        self.fc_img = nn.Linear(feat_dim, hid_dim)\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        img_feat = self.fc_img(img_feat).unsqueeze(1)         # (B, 1, H)\n",
    "        cap_embeds = self.embed(captions)                     # (B, T, E)\n",
    "        x = torch.cat([img_feat, cap_embeds[:, :-1, :]], dim=1)  # Shifted input\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc_out(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a25fc6-1804-4621-94d4-87011d6a76fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "PAD_IDX = 0  # Replace with your actual padding token index\n",
    "model = Captioner()\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2ca66a-f4f6-4edf-a8f2-498afa1a892c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0. Imports & Cleanup\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, re, random, requests, shutil, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "from PIL import Image, UnidentifiedImageError\n",
    "\n",
    "# Clean previous data\n",
    "shutil.rmtree(\"images\", ignore_errors=True)\n",
    "shutil.rmtree(\"preproc\", ignore_errors=True)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Setup: Download or Use Local Image\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "IMAGE_DIR = Path(\"images\")\n",
    "FEATURE_DIR = Path(\"preproc\")\n",
    "MANIFEST = FEATURE_DIR / \"manifest.jsonl\"\n",
    "IMAGE_DIR.mkdir(exist_ok=True)\n",
    "FEATURE_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "img_path = IMAGE_DIR / \"demo.jpg\"\n",
    "img_url = \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131?w=640\"\n",
    "\n",
    "def download_image_safe(url, path):\n",
    "    try:\n",
    "        print(\"\\U0001F4E5 Attempting to download image...\")\n",
    "        r = requests.get(url, timeout=10)\n",
    "        if r.status_code == 200 and len(r.content) > 10000:\n",
    "            with open(path, \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "            Image.open(path).verify()\n",
    "            print(f\"âœ… Valid image saved at: {path}\")\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Download failed: {e}\")\n",
    "        return False\n",
    "\n",
    "image_ready = False\n",
    "if not img_path.exists() or img_path.stat().st_size < 10000:\n",
    "    image_ready = download_image_safe(img_url, img_path)\n",
    "\n",
    "if not image_ready:\n",
    "    print(\"âš ï¸  Fallback: searching local .jpg image...\")\n",
    "    for file in IMAGE_DIR.glob(\"*.jpg\"):\n",
    "        try:\n",
    "            Image.open(file).verify()\n",
    "            img_path = file\n",
    "            print(f\"âœ… Found local image: {img_path}\")\n",
    "            image_ready = True\n",
    "            break\n",
    "        except UnidentifiedImageError:\n",
    "            continue\n",
    "\n",
    "if not image_ready:\n",
    "    raise RuntimeError(\"âŒ No valid image. Place a .jpg file in 'images/' folder named 'demo.jpg'\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Feature Extraction + Manifest Creation\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def process_dataset(image_dir, manifest_path, feature_dir):\n",
    "    model = resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    manifest = []\n",
    "    for img_path in image_dir.glob(\"*.jpg\"):\n",
    "        try:\n",
    "            img = Image.open(img_path).convert(\"RGB\")\n",
    "        except UnidentifiedImageError:\n",
    "            print(f\"âŒ Skipping unreadable image: {img_path}\")\n",
    "            continue\n",
    "        tensor = transform(img).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            feat = model(tensor).squeeze().clone()\n",
    "        feat_path = feature_dir / f\"{img_path.stem}.pt\"\n",
    "        torch.save(feat, feat_path)\n",
    "        manifest.append({\"feature\": feat_path.name, \"captions\": [\"a photo of a cat\"]})\n",
    "\n",
    "    with open(manifest_path, \"w\") as f:\n",
    "        for entry in manifest:\n",
    "            f.write(json.dumps(entry) + \"\\n\")\n",
    "    print(\"âœ… Manifest created with\", len(manifest), \"entries.\")\n",
    "\n",
    "process_dataset(IMAGE_DIR, MANIFEST, FEATURE_DIR)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Vocabulary Class\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=1):\n",
    "        self.special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.itos = self.special.copy()\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.special)}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def _tokenize(self, text):\n",
    "        return re.findall(r\"\\w+\", text.lower())\n",
    "\n",
    "    def build(self, captions):\n",
    "        freqs = Counter(tok for cap in captions for tok in self._tokenize(cap))\n",
    "        for word, freq in freqs.items():\n",
    "            if freq >= self.min_freq and word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi.get(tok, self.stoi[\"<unk>\"]) for tok in self._tokenize(text)]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join(self.itos[i] for i in ids if i not in {\n",
    "            self.stoi[\"<pad>\"], self.stoi[\"<start>\"], self.stoi[\"<end>\"]\n",
    "        })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Load Manifest + Build Vocabulary\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(MANIFEST) as f:\n",
    "    rows = [json.loads(line) for line in f if \"feature\" in line]\n",
    "\n",
    "if not rows:\n",
    "    raise ValueError(\"âŒ No valid samples found.\")\n",
    "\n",
    "for row in rows:\n",
    "    row[\"captions\"] = row.get(\"captions\", [\"a photo\"])\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.build([cap for r in rows for cap in r[\"captions\"]])\n",
    "print(\"âœ… Vocab size:\", len(vocab))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. Dataset and DataLoader\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, manifest_rows, vocab, max_len=20):\n",
    "        self.rows = manifest_rows\n",
    "        self.vocab = vocab\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.rows)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.rows[idx]\n",
    "        feat = torch.load(FEATURE_DIR / row[\"feature\"])\n",
    "        cap = random.choice(row[\"captions\"]).lower()\n",
    "        ids = [self.vocab.stoi[\"<start>\"]] + self.vocab.encode(cap)[:self.max_len] + [self.vocab.stoi[\"<end>\"]]\n",
    "        return feat, torch.tensor(ids)\n",
    "\n",
    "def collate(batch):\n",
    "    feats, caps = zip(*batch)\n",
    "    feats = torch.stack(feats)\n",
    "    caps = pad_sequence(caps, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n",
    "    return feats, caps\n",
    "\n",
    "train_ds = CaptionDataset(rows, vocab)\n",
    "train_dl = DataLoader(train_ds, batch_size=4, shuffle=True, collate_fn=collate)\n",
    "print(\"âœ… Dataset size:\", len(train_ds))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. Captioner Model\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=512, vocab_size=len(vocab)):\n",
    "        super().__init__()\n",
    "        self.img_fc = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=vocab.stoi[\"<pad>\"])\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h0 = torch.tanh(self.img_fc(feats)).unsqueeze(0)\n",
    "        x = self.emb(caps)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. Training Loop\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Captioner().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<pad>\"])\n",
    "\n",
    "model.train()\n",
    "for epoch in range(10):\n",
    "    total_loss = 0\n",
    "    for feats, caps in train_dl:\n",
    "        feats, caps = feats.to(device), caps.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(feats, caps)\n",
    "        targets = caps[:, 1:]\n",
    "        outputs = outputs[:, :-1, :]\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(-1)), targets.reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"ğŸ“˜ Epoch [{epoch+1}/10] â€” Loss: {total_loss / len(train_dl):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7cb4c-2f23-46f7-9c91-f235ebfa1533",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ad26b-7f47-4a29-b6e0-3aac1d4f54f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a2b2a6-44a4-42ad-8eea-243a900c23a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4aec6d-ee94-4148-9400-91010b191ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ResNet18 and remove the classifier layer\n",
    "resnet = models.resnet18(pretrained=True)\n",
    "resnet = torch.nn.Sequential(*(list(resnet.children())[:-1]))\n",
    "resnet.eval()\n",
    "\n",
    "# Transform image to fit ResNet input\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "# Load your image\n",
    "img_path = \"photo_2025-06-09_21-10-49.jpg\"  # file already in your folder\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "img = transform(img).unsqueeze(0)  # Add batch dim\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    features = resnet(img).squeeze().flatten()  # Shape: (512,)\n",
    "\n",
    "# Convert to dataset-style tensor\n",
    "img_features = features.unsqueeze(0)  # Shape: (1, 512)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef7b8f-c540-471d-a8ca-b2c067165032",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_captions = torch.tensor([[1, 5, 10, 3]])  # Example: [<start>, 'a', 'cat', <end>]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef6acf4-d2e9-4df5-ab1e-1295f5046e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "# Create Dataset and DataLoader\n",
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7abf2b91-da76-4f92-8289-983ce779cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.fc = nn.Linear(512, 1000)  # change 1000 to vocab size\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        return self.fc(img_feats)  # dummy forward for now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668c25eb-1101-469c-a611-4d80f2f59a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Captioner()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7f1bcf-6abc-46de-8585-b4866ff1e3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e870619-ec92-4b48-9cfa-8ce6aaeb7226",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa050a95-c14f-4c23-aba6-9fa1347e85c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0000386-b51b-4122-9f80-3cbe3603f44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # Or any number you choose\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for img_feats, captions in train_dl:\n",
    "        img_feats = img_feats.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Input: captions without last token (e.g. <start> A cat)\n",
    "        # Target: captions without first token (e.g. A cat <end>)\n",
    "        inputs = captions[:, :-1]     # (B, T-1)\n",
    "        targets = captions[:, 1:]     # (B, T-1)\n",
    "\n",
    "        outputs = model(img_feats, inputs)  # (B, T-1, V)\n",
    "\n",
    "        # Reshape to match CrossEntropyLoss input expectations\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))  # (B*T-1, V)\n",
    "        targets = targets.reshape(-1)                    # (B*T-1)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"ğŸ“˜ Epoch [{epoch+1}/{num_epochs}] â€” Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc3e708-52cb-482b-bd2b-3fd2e4e71c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "\n",
    "    for img_feats, captions in train_dl:\n",
    "        img_feats = img_feats.to(device)     # (B, 2048)\n",
    "        captions = captions.to(device)       # (B, T)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Input to model: all tokens except last (teacher forcing)\n",
    "        inputs = captions[:, :-1]            # (B, T-1)\n",
    "        targets = captions[:, 1:]            # (B, T-1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(img_feats, inputs)   # (B, T-1, vocab_size)\n",
    "\n",
    "        # Flatten for loss computation\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))  # (B*(T-1), vocab_size)\n",
    "        targets = targets.reshape(-1)                    # (B*(T-1))\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_dl)\n",
    "    print(f\"ğŸ“˜ Epoch [{epoch+1}/{num_epochs}] â€” Loss: {avg_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c8acc5-b118-43d1-a652-eab16dd2a9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"outputs shape:\", outputs.shape)  # should be (B*T, V)\n",
    "print(\"targets shape:\", targets.shape)  # should be (B*T,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157cdb18-0c42-4cc3-9fe6-cd619df37add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # img_feats: (batch, feature_dim)\n",
    "        # captions: (batch, seq_len)\n",
    "\n",
    "        embeddings = self.embedding(captions)  # (batch, seq_len, embed_size)\n",
    "\n",
    "        # You can optionally expand and feed image features as first input token\n",
    "        # or use it to initialize hidden state\n",
    "\n",
    "        outputs, _ = self.lstm(embeddings)     # (batch, seq_len, hidden_size)\n",
    "        outputs = self.fc(outputs)             # (batch, seq_len, vocab_size)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b71ce72-f714-4cc2-b9a8-87f54f59e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(img_feats, captions)   # (B, T, V)\n",
    "outputs = outputs[:, 1:, :]            # skip <start> prediction if needed\n",
    "targets = captions[:, 1:]              # shift ground truth\n",
    "\n",
    "outputs = outputs.reshape(-1, outputs.shape[-1])  # (B*T, V)\n",
    "targets = targets.reshape(-1)                     # (B*T,)\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82658621-78ca-410e-9fdc-13a7d8d81f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        \"\"\"\n",
    "        img_feats is unused here but you can use it to init hidden state\n",
    "        captions: [batch_size, seq_len]\n",
    "        \"\"\"\n",
    "        embeddings = self.embedding(captions)                 # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(embeddings)                   # (B, T, H)\n",
    "        outputs = self.linear(lstm_out)                       # (B, T, V)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9121e7b9-3139-4860-85fc-49c90ff2f6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(img_feats, captions)        # (B, T, V)\n",
    "outputs = outputs[:, 1:, :]                 # skip <start> token\n",
    "targets = captions[:, 1:]                   # predict next word\n",
    "\n",
    "outputs = outputs.reshape(-1, outputs.size(-1))  # (B*T, V)\n",
    "targets = targets.reshape(-1)                   # (B*T)\n",
    "\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a187a4a-d7dc-473a-a1bd-b67789466772",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # Use img_feats later if needed for context initialization\n",
    "        embeddings = self.embedding(captions)            # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(embeddings)              # (B, T, H)\n",
    "        outputs = self.linear(lstm_out)                  # (B, T, V)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decb52fd-5413-4de0-b597-e23bbd47ae36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feats, captions):\n",
    "        # img_feats is not used in this simple version; can be added to init LSTM hidden\n",
    "        embeddings = self.embedding(captions)         # (B, T, E)\n",
    "        lstm_out, _ = self.lstm(embeddings)           # (B, T, H)\n",
    "        outputs = self.linear(lstm_out)               # (B, T, V)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0be9d3-c223-4011-8778-d97957a4401f",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [[\"a\", \"dog\", \"on\", \"a\", \"beach\"], [\"a\", \"cat\", \"on\", \"a\", \"mat\"], ...]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd5345-5661-4290-b927-456cab1d2ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = [\n",
    "    \"<START> a dog is playing with a ball <END>\",\n",
    "    \"<START> a child is eating ice cream <END>\",\n",
    "    \"<START> a man is riding a bicycle <END>\"\n",
    "]\n",
    "\n",
    "with open(\"captions.txt\", \"w\") as f:\n",
    "    for caption in captions:\n",
    "        f.write(caption + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71de87c-932e-4552-b80c-d93b8f2681f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "captions = []\n",
    "\n",
    "with open(\"captions.txt\", \"r\") as file:\n",
    "    for line in file:\n",
    "        line = line.strip()\n",
    "        if line:  # skip empty lines\n",
    "            tokens = line.lower().split()  # basic tokenization\n",
    "            captions.append(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbeac97f-a3e5-4a7e-8ac7-8913ffcf8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten all tokens\n",
    "all_tokens = [token for caption in captions for token in caption]\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Set a threshold frequency (e.g., keep words appearing â‰¥1 time)\n",
    "threshold = 1\n",
    "vocab = [word for word, count in counter.items() if count >= threshold]\n",
    "\n",
    "# Special tokens\n",
    "vocab = ['<PAD>', '<START>', '<END>', '<UNK>'] + sorted(set(vocab))\n",
    "word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "vocab_size = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42d72f9-eb04-4763-b03c-340b499cca9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(len(cap) for cap in captions)\n",
    "\n",
    "encoded_captions = []\n",
    "for caption in captions:\n",
    "    encoded = [word2idx.get(word, word2idx['<UNK>']) for word in caption]\n",
    "    # Pad to max_len\n",
    "    encoded += [word2idx['<PAD>']] * (max_len - len(encoded))\n",
    "    encoded_captions.append(encoded)\n",
    "\n",
    "encoded_captions = torch.tensor(encoded_captions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d4f9e0-7081-4331-9753-bc808cff0dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Example image features (e.g., extracted by ResNet)\n",
    "img_features = torch.randn(10, 2048)  # 10 samples, 2048-dim features\n",
    "\n",
    "# Captions for each image (1:1 mapping)\n",
    "captions = [\n",
    "    \"A cat sitting on a mat\",\n",
    "    \"A dog running in the park\",\n",
    "    \"A child playing with a ball\",\n",
    "    \"A man riding a bicycle\",\n",
    "    \"A woman reading a book\",\n",
    "    \"A bird flying in the sky\",\n",
    "    \"A car parked on the road\",\n",
    "    \"A group of people hiking\",\n",
    "    \"A boat floating on water\",\n",
    "    \"A cityscape with tall buildings\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Simple Vocabulary\n",
    "# -----------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.itos = self.special.copy()\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.special)}\n",
    "\n",
    "    def build(self, captions):\n",
    "        counter = Counter()\n",
    "        for cap in captions:\n",
    "            counter.update(re.findall(r\"\\w+\", cap.lower()))\n",
    "        for word in counter:\n",
    "            if word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [\"<start>\"] + re.findall(r\"\\w+\", text.lower()) + [\"<end>\"]\n",
    "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.build(captions)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Encode captions\n",
    "# -----------------------------\n",
    "encoded_captions = [torch.tensor(vocab.encode(cap)) for cap in captions]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Pad Function\n",
    "# -----------------------------\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img_feats, caps = zip(*batch)\n",
    "    img_feats = torch.stack(img_feats)\n",
    "    caps = pad_sequence(caps, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n",
    "    return img_feats, caps\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Dataset + DataLoader\n",
    "# -----------------------------\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Test loading\n",
    "# -----------------------------\n",
    "for img_feats, caps in train_dl:\n",
    "    print(\"Image Feature Shape:\", img_feats.shape)  # (B, 2048)\n",
    "    print(\"Caption Shape:\", caps.shape)             # (B, T)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794b352-25de-41a4-b6ce-30e02b35311a",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "\n",
    "model = Captioner(embed_size, hidden_size, vocab_size)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=word2idx['<PAD>'])\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc305663-a0ad-4989-8a8b-b7e2f727e70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from collections import Counter\n",
    "import re\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# -----------------------------\n",
    "# 1. Dummy Image Features (10 samples)\n",
    "# -----------------------------\n",
    "img_features = torch.randn(10, 2048)  # Shape: (10, 2048)\n",
    "\n",
    "# 2. Dummy Captions\n",
    "captions = [\n",
    "    \"A cat sitting on a mat\",\n",
    "    \"A dog running in the park\",\n",
    "    \"A child playing with a ball\",\n",
    "    \"A man riding a bicycle\",\n",
    "    \"A woman reading a book\",\n",
    "    \"A bird flying in the sky\",\n",
    "    \"A car parked on the road\",\n",
    "    \"A group of people hiking\",\n",
    "    \"A boat floating on water\",\n",
    "    \"A cityscape with tall buildings\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Vocabulary Builder\n",
    "# -----------------------------\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.special = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.itos = self.special.copy()\n",
    "        self.stoi = {tok: idx for idx, tok in enumerate(self.special)}\n",
    "\n",
    "    def build(self, captions):\n",
    "        counter = Counter()\n",
    "        for cap in captions:\n",
    "            counter.update(re.findall(r\"\\w+\", cap.lower()))\n",
    "        for word in counter:\n",
    "            if word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "\n",
    "    def encode(self, text):\n",
    "        tokens = [\"<start>\"] + re.findall(r\"\\w+\", text.lower()) + [\"<end>\"]\n",
    "        return [self.stoi.get(t, self.stoi[\"<unk>\"]) for t in tokens]\n",
    "\n",
    "vocab = Vocabulary()\n",
    "vocab.build(captions)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Encode Captions\n",
    "# -----------------------------\n",
    "encoded_captions = [torch.tensor(vocab.encode(cap)) for cap in captions]\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Dataset & DataLoader\n",
    "# -----------------------------\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, image_features, captions):\n",
    "        self.image_features = image_features\n",
    "        self.captions = captions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.captions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.image_features[idx], self.captions[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    img_feats, caps = zip(*batch)\n",
    "    img_feats = torch.stack(img_feats)\n",
    "    caps = pad_sequence(caps, batch_first=True, padding_value=vocab.stoi[\"<pad>\"])\n",
    "    return img_feats, caps\n",
    "\n",
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Print Shapes\n",
    "# -----------------------------\n",
    "print(\"Image features shape:\", img_features.shape)\n",
    "print(\"Number of encoded captions:\", len(encoded_captions))\n",
    "print(\"Example caption tensor shape:\", encoded_captions[0].shape)\n",
    "\n",
    "# Optional: Print 1 batch\n",
    "for img_feats_batch, caps_batch in train_dl:\n",
    "    print(\"\\nBatch image features shape:\", img_feats_batch.shape)  # (B, 2048)\n",
    "    print(\"Batch caption shape:\", caps_batch.shape)                # (B, T)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02ebb45-d4dd-4b40-b8b8-18621c8b45ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix shape mismatch: make image features match captions\n",
    "img_features = img_features.repeat(3, 1)  # Now shape is [3, 512]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38db955-23ee-4856-bd74-e04e021b2a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ImageCaptionDataset(img_features, encoded_captions)\n",
    "train_dl = DataLoader(train_dataset, batch_size=2, shuffle=True)  # batch_size can be 2 or 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9ddb8a-59ce-4821-95d4-587e98ff30a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10  # you can increase later if needed\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for img_feats, captions in train_dl:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(img_feats, captions)       # Shape: (B, T, V)\n",
    "        outputs = outputs[:, 1:, :]                 # Skip <START>\n",
    "        targets = captions[:, 1:]                   # Next word prediction\n",
    "\n",
    "        outputs = outputs.reshape(-1, outputs.size(-1))  # (B*T, V)\n",
    "        targets = targets.reshape(-1)                    # (B*T)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef38083-ca4d-4d07-b1a1-736c2e573e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)  # (1, T)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)  # (1, T, V)\n",
    "            next_word_logits = output[0, -1]  # last token's output\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "\n",
    "            caption.append(predicted)\n",
    "\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "\n",
    "    # Decode\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab[idx] for idx in caption[1:-1]])  # Skip <START> and <END>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0527b3b7-cbc7-4504-bcdd-c28ece68efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Flatten all tokens\n",
    "all_tokens = [token for caption in captions for token in caption]\n",
    "\n",
    "# Count frequency\n",
    "counter = Counter(all_tokens)\n",
    "\n",
    "# Set a threshold if needed, e.g., min_freq = 1\n",
    "tokens = sorted(counter)\n",
    "\n",
    "# Build vocab dict\n",
    "vocab = {token: idx + 2 for idx, token in enumerate(tokens)}\n",
    "vocab['<PAD>'] = 0\n",
    "vocab['<START>'] = 1\n",
    "vocab['<END>'] = len(vocab)  # or any fixed value\n",
    "\n",
    "# OPTIONAL: Create inverse vocab for decoding\n",
    "inv_vocab = {idx: token for token, idx in vocab.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa072f3-94aa-4685-8c1a-3d4e531470e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Dummy Captioner model\n",
    "# -----------------------------\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=512, vocab_size=100):\n",
    "        super().__init__()\n",
    "        self.img_fc = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feat, caps):\n",
    "        h0 = torch.tanh(self.img_fc(feat)).unsqueeze(0)\n",
    "        x = self.emb(caps)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# -----------------------------\n",
    "# Caption Generation Function\n",
    "# -----------------------------\n",
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    generated = [vocab.stoi[\"<start>\"]]\n",
    "    img_feat = img_feat.unsqueeze(0)  # add batch dim\n",
    "\n",
    "    with torch.no_grad():\n",
    "        h = torch.tanh(model.img_fc(img_feat)).unsqueeze(0)\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            input_ids = torch.tensor([generated], dtype=torch.long)  # shape: (1, T)\n",
    "            emb = model.emb(input_ids)\n",
    "            output, h = model.gru(emb, h)\n",
    "            logits = model.fc_out(output[:, -1, :])  # last token output\n",
    "            pred = logits.argmax(dim=-1).item()\n",
    "            generated.append(pred)\n",
    "            if pred == vocab.stoi[\"<end>\"]:\n",
    "                break\n",
    "\n",
    "    # Decode using vocab.itos\n",
    "    caption_words = [vocab.itos[idx] for idx in generated[1:-1] if idx < len(vocab.itos)]\n",
    "    return ' '.join(caption_words)\n",
    "\n",
    "# -----------------------------\n",
    "# Usage (Assuming trained model & vocab)\n",
    "# -----------------------------\n",
    "# model = Captioner(...).to(device)\n",
    "# img_feat = img_features[0].to(device)\n",
    "# caption = generate_caption(model, img_feat, vocab)\n",
    "# print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cf8fd0-2b98-473e-b4b1-90e4ea11d70e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab['<UNK>'] = len(vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3a2f26-ed1e-4681-a5fb-106e882d7b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to decode a caption (list of token indices) into text\n",
    "def decode_caption(caption, vocab):\n",
    "    \"\"\"\n",
    "    caption: list of token indices\n",
    "    vocab: Vocabulary object with vocab.itos (index to string)\n",
    "    \"\"\"\n",
    "    # Skip <start> and <end> tokens\n",
    "    words = [vocab.itos[idx] if idx < len(vocab.itos) else '<UNK>' for idx in caption[1:-1]]\n",
    "    return ' '.join(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77926758-6636-4b3b-aacc-f3c6ffb48872",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)  # (1, T)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)  # (1, T, V)\n",
    "            next_word_logits = output[0, -1]  # last token's output\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "\n",
    "    # Create reverse vocab\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "\n",
    "    # âœ… This line must be inside the function!\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190cb86f-e67b-4feb-8def-90b6d4301ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption(model, img_features[0], vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24127591-be5f-428e-a562-279424c610b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)  # shape (1, T)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)  # shape (1, T, V)\n",
    "            next_word_logits = output[0, -1]  # last timestep\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "\n",
    "    # Decode caption (skip <START> and <END>)\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e95460-4045-4025-b131-c45a24affa03",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = generate_caption(model, img_features[0], vocab)\n",
    "print(\"Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fd6d2-42b4-4ee6-b228-7f8f3e809644",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(img_features)):\n",
    "    caption = generate_caption(model, img_features[i], vocab)\n",
    "    print(f\"Image {i+1} Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ff76e4-5c31-47be-906b-7d52698b70f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "torch.save(model.state_dict(), 'caption_model.pth')\n",
    "\n",
    "# Load\n",
    "model.load_state_dict(torch.load('caption_model.pth'))\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7734e744-325f-4685-b22d-574945b0672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a76a66-0c10-4230-8f36-b762adf7ae8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3c992ae-dad2-4d78-877b-59f8ef12e6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… COMPLETE WORKING CODE (Training + Saving + Streamlit)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PART 1: TRAINING + SAVING VOCAB & MODEL\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "import os, json, re, pickle, torch, random\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision import transforms, models\n",
    "\n",
    "# Directories\n",
    "Path(\"images\").mkdir(exist_ok=True)\n",
    "Path(\"preproc\").mkdir(exist_ok=True)\n",
    "\n",
    "# Dummy image for training\n",
    "img_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/3/3a/Cat03.jpg/640px-Cat03.jpg\"\n",
    "img_path = Path(\"images/demo.jpg\")\n",
    "\n",
    "if not img_path.exists():\n",
    "    import requests\n",
    "    r = requests.get(img_url)\n",
    "    with open(img_path, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€ 1. Vocabulary â”€â”€â”€â”€â”€â”€â”€\n",
    "class Vocabulary:\n",
    "    def __init__(self):\n",
    "        self.itos = [\"<pad>\", \"<start>\", \"<end>\", \"<unk>\"]\n",
    "        self.stoi = {tok: i for i, tok in enumerate(self.itos)}\n",
    "\n",
    "    def build(self, captions):\n",
    "        freqs = Counter(word for cap in captions for word in re.findall(r\"\\w+\", cap.lower()))\n",
    "        for word, freq in freqs.items():\n",
    "            if word not in self.stoi:\n",
    "                self.stoi[word] = len(self.itos)\n",
    "                self.itos.append(word)\n",
    "\n",
    "    def encode(self, text):\n",
    "        return [self.stoi.get(w, self.stoi[\"<unk>\"]) for w in re.findall(r\"\\w+\", text.lower())]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join(self.itos[i] for i in ids if i not in {self.stoi[\"<pad>\"], self.stoi[\"<start>\"], self.stoi[\"<end>\"]})\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€ 2. Feature Extractor â”€â”€â”€â”€â”€â”€â”€\n",
    "def extract_feature(img_path):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img_tensor = transform(img).unsqueeze(0)\n",
    "    model = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "    model = nn.Sequential(*list(model.children())[:-1])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor).squeeze()\n",
    "    return features\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€ 3. Captioner Model â”€â”€â”€â”€â”€â”€â”€\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=512, vocab_size=10000):\n",
    "        super().__init__()\n",
    "        self.img_fc = nn.Linear(feat_dim, hid_dim)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "\n",
    "    def forward(self, feats, caps):\n",
    "        h0 = torch.tanh(self.img_fc(feats)).unsqueeze(0)\n",
    "        x = self.emb(caps)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        return self.fc_out(out)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€ 4. Training â”€â”€â”€â”€â”€â”€â”€\n",
    "caption = \"A photo of a cat\"\n",
    "vocab = Vocabulary()\n",
    "vocab.build([caption])\n",
    "encoded = [vocab.stoi[\"<start>\"]] + vocab.encode(caption) + [vocab.stoi[\"<end>\"]]\n",
    "feature = extract_feature(img_path)\n",
    "\n",
    "class DummyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.feat = feature\n",
    "        self.cap = torch.tensor(encoded)\n",
    "    def __len__(self): return 10\n",
    "    def __getitem__(self, idx): return self.feat, self.cap\n",
    "\n",
    "def collate_fn(batch):\n",
    "    feats, caps = zip(*batch)\n",
    "    return torch.stack(feats), pad_sequence(caps, batch_first=True, padding_value=0)\n",
    "\n",
    "dataloader = DataLoader(DummyDataset(), batch_size=2, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "model = Captioner(vocab_size=len(vocab.itos))\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "model.train()\n",
    "for epoch in range(2):\n",
    "    total_loss = 0\n",
    "    for feats, caps in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(feats, caps[:, :-1])\n",
    "        loss = criterion(outputs.reshape(-1, outputs.size(2)), caps[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€ 5. Save â”€â”€â”€â”€â”€â”€â”€\n",
    "torch.save(model.state_dict(), \"caption_model.pth\")\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump({\"itos\": vocab.itos, \"stoi\": vocab.stoi}, f)\n",
    "\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# PART 2: STREAMLIT APP\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Save this part separately as app.py and run using: streamlit run app.py\n",
    "\n",
    "# import streamlit as st\n",
    "# from PIL import Image\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from torchvision import models, transforms\n",
    "# import pickle, os\n",
    "# \n",
    "# class Vocabulary:\n",
    "#     def __init__(self):\n",
    "#         self.itos = []\n",
    "#         self.stoi = {}\n",
    "#     def load(self, path):\n",
    "#         with open(path, \"rb\") as f:\n",
    "#             data = pickle.load(f)\n",
    "#             self.itos = data[\"itos\"]\n",
    "#             self.stoi = data[\"stoi\"]\n",
    "# \n",
    "# class Captioner(nn.Module):\n",
    "#     def __init__(self, feat_dim=2048, emb_dim=256, hid_dim=512, vocab_size=10000):\n",
    "#         super().__init__()\n",
    "#         self.img_fc = nn.Linear(feat_dim, hid_dim)\n",
    "#         self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "#         self.gru = nn.GRU(emb_dim, hid_dim, batch_first=True)\n",
    "#         self.fc_out = nn.Linear(hid_dim, vocab_size)\n",
    "#     def forward(self, feats, caps):\n",
    "#         h0 = torch.tanh(self.img_fc(feats)).unsqueeze(0)\n",
    "#         x = self.emb(caps)\n",
    "#         out, _ = self.gru(x, h0)\n",
    "#         return self.fc_out(out)\n",
    "# \n",
    "# def extract_features(img_tensor):\n",
    "#     resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "#     resnet = nn.Sequential(*list(resnet.children())[:-1])\n",
    "#     resnet.eval()\n",
    "#     with torch.no_grad():\n",
    "#         return resnet(img_tensor).squeeze()\n",
    "# \n",
    "# def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "#     model.eval()\n",
    "#     generated = [vocab.stoi[\"<start>\"]]\n",
    "#     with torch.no_grad():\n",
    "#         h = torch.tanh(model.img_fc(img_feat.unsqueeze(0))).unsqueeze(0)\n",
    "#         for _ in range(max_len):\n",
    "#             input_ids = torch.tensor([generated], dtype=torch.long)\n",
    "#             output, h = model.gru(model.emb(input_ids), h)\n",
    "#             logits = model.fc_out(output[:, -1, :])\n",
    "#             next_word = logits.argmax(dim=-1).item()\n",
    "#             generated.append(next_word)\n",
    "#             if next_word == vocab.stoi[\"<end>\"]:\n",
    "#                 break\n",
    "#     return \" \".join([vocab.itos[i] for i in generated[1:-1]])\n",
    "# \n",
    "# st.title(\"ğŸ–¼ï¸ Image Caption Generator\")\n",
    "# if not os.path.exists(\"vocab.pkl\") or not os.path.exists(\"caption_model.pth\"):\n",
    "#     st.error(\"âŒ Model or vocab file missing. Please train first.\")\n",
    "#     st.stop()\n",
    "# \n",
    "# vocab = Vocabulary()\n",
    "# vocab.load(\"vocab.pkl\")\n",
    "# model = Captioner(vocab_size=len(vocab.itos))\n",
    "# model.load_state_dict(torch.load(\"caption_model.pth\", map_location=\"cpu\"))\n",
    "# model.eval()\n",
    "# \n",
    "# transform = transforms.Compose([\n",
    "#     transforms.Resize((224, 224)),\n",
    "#     transforms.ToTensor(),\n",
    "#     transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "# ])\n",
    "# \n",
    "# uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
    "# if uploaded_file is not None:\n",
    "#     image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "#     st.image(image, caption=\"Uploaded Image\", use_column_width=True)\n",
    "#     img_tensor = transform(image).unsqueeze(0)\n",
    "#     img_feat = extract_features(img_tensor)\n",
    "#     caption = generate_caption(model, img_feat, vocab)\n",
    "#     st.markdown(f\"**Generated Caption:** {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9593e6c9-4e48-49d5-8324-cbb2bbbf891e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        embeddings = self.embed(captions)  # (B, T, E)\n",
    "        img_feat = img_feat.unsqueeze(1)   # (B, 1, E)\n",
    "        embeddings = torch.cat((img_feat, embeddings), 1)  # (B, T+1, E)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9583f-4cf2-498c-84d7-aacf3eb26dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import pickle\n",
    "\n",
    "def extract_features(img_tensor):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.squeeze(0)\n",
    "\n",
    "def load_vocab(path='vocab.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)\n",
    "            next_word_logits = output[0, -1]\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5820cd9-9be1-4791-9c1a-649815b68ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet18\n",
    "import pickle\n",
    "\n",
    "def extract_features(img_tensor):\n",
    "    model = resnet18(pretrained=True)\n",
    "    model.fc = torch.nn.Identity()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        features = model(img_tensor)\n",
    "    return features.squeeze(0)\n",
    "\n",
    "def load_vocab(path='vocab.pkl'):\n",
    "    with open(path, 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    return vocab\n",
    "\n",
    "def generate_caption(model, img_feat, vocab, max_len=20):\n",
    "    model.eval()\n",
    "    caption = [vocab['<START>']]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_len):\n",
    "            cap_tensor = torch.tensor(caption).unsqueeze(0)\n",
    "            output = model(img_feat.unsqueeze(0), cap_tensor)\n",
    "            next_word_logits = output[0, -1]\n",
    "            predicted = next_word_logits.argmax().item()\n",
    "            caption.append(predicted)\n",
    "            if predicted == vocab['<END>']:\n",
    "                break\n",
    "    inv_vocab = {idx: tok for tok, idx in vocab.items()}\n",
    "    return ' '.join([inv_vocab.get(idx, '<UNK>') for idx in caption[1:-1]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ffea43-e1d5-4107-84f0-769755273e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Captioner(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size):\n",
    "        super(Captioner, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, img_feat, captions):\n",
    "        embeddings = self.embed(captions)  # (B, T, E)\n",
    "        img_feat = img_feat.unsqueeze(1)   # (B, 1, E)\n",
    "        embeddings = torch.cat((img_feat, embeddings), 1)  # (B, T+1, E)\n",
    "        hiddens, _ = self.lstm(embeddings)\n",
    "        outputs = self.fc(hiddens)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2fc0151-8c37-4574-aa6a-18d5737c56b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"captions.txt\", \"w\") as f:\n",
    "    f.write(\"image1.jpg A man riding a bike.\\n\")\n",
    "    f.write(\"image2.jpg A cat sleeping on the bed.\\n\")\n",
    "    f.write(\"image3.jpg A group of people playing football.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f22cb-7e94-4475-9e25-beee2c16494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    image_name, caption = line.strip().split(' ', 1)\n",
    "    print(f\"Image: {image_name}, Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27de14d4-1c61-46b7-8cb6-da685949f691",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Read captions\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "captions = [line.strip().split(' ', 1)[1] for line in lines]\n",
    "\n",
    "# Tokenize and count words\n",
    "all_words = []\n",
    "for caption in captions:\n",
    "    all_words.extend(caption.lower().split())\n",
    "\n",
    "# Count word frequency\n",
    "word_counts = Counter(all_words)\n",
    "\n",
    "# Optionally, create word2idx and idx2word\n",
    "vocab = {word: idx for idx, (word, _) in enumerate(word_counts.items(), start=1)}\n",
    "\n",
    "print(\"Vocabulary:\", vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243554e2-58b1-4285-8148-abafc8dd4c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "for line in lines:\n",
    "    image_name, caption = line.strip().split(' ', 1)\n",
    "    image_path = os.path.join(\"images\", image_name)  # replace \"images\" with your folder path\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        img = Image.open(image_path)\n",
    "        img.show()  # just to verify it loads\n",
    "    else:\n",
    "        print(f\"Image {image_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d93a990-0e32-461b-8648-2c4d85105afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = os.path.join(\"images\", image_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1c42a7-3306-4729-a7a5-7c4d56624a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Read captions from file\n",
    "with open(\"captions.txt\", \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if \"|\" not in line:\n",
    "        continue  # skip malformed lines\n",
    "    image_name, caption = line.strip().split('|', 1)\n",
    "    image_path = os.path.join(\"images\", image_name)\n",
    "\n",
    "    if os.path.exists(image_path):\n",
    "        print(f\"\\nâœ… Showing: {image_name} - {caption}\")\n",
    "        img = Image.open(image_path)\n",
    "        img.show()\n",
    "    else:\n",
    "        print(f\"\\nâŒ Image {image_name} not found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3a538d-f98f-445d-8a44-23dcd3d4078d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import os\n",
    "\n",
    "# Define path to your image\n",
    "img_path = \"images/demo.jpg\"  # âœ… Make sure this image file exists at this path\n",
    "\n",
    "# Check if image exists\n",
    "if not os.path.exists(img_path):\n",
    "    raise FileNotFoundError(f\"âŒ Image not found at {img_path}. Please check the path.\")\n",
    "\n",
    "# Open and display the image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "display(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8a9fd-a22d-4c7e-83b6-e562cad0b94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bc37d3-12e3-4b06-8868-ed9bf163adf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56276289-9095-47e5-956a-6498952e0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(img):\n",
    "    # Load pretrained segmentation model\n",
    "    model = deeplabv3_resnet101(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocessing\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    input_tensor = preprocess(img).unsqueeze(0)  # add batch dimension\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "        output_predictions = output.argmax(0)\n",
    "\n",
    "    # Convert to PIL Image for display\n",
    "    segmented_img = Image.fromarray(output_predictions.byte().cpu().numpy())\n",
    "    return segmented_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b65b38a-96fd-4dba-9e21-f21db58e8fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "# Step 1: Load image\n",
    "img_path = \"images/demo.jpg\"\n",
    "\n",
    "# Check image exists\n",
    "if not os.path.exists(img_path):\n",
    "    raise FileNotFoundError(f\"âŒ Image not found at {img_path}. Please check the path.\")\n",
    "\n",
    "# Open image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "print(\"ğŸ–¼ï¸ Original Image:\")\n",
    "display(img)\n",
    "\n",
    "# Step 2: Define segmentation function\n",
    "def segment_image(img):\n",
    "    # Load model\n",
    "    model = deeplabv3_resnet101(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    # Preprocess\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    input_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    # Predict segmentation\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "        output_predictions = output.argmax(0)\n",
    "\n",
    "    # Convert to PIL Image\n",
    "    segmented_img = Image.fromarray(output_predictions.byte().cpu().numpy())\n",
    "    return segmented_img\n",
    "\n",
    "# Step 3: Segment and display\n",
    "segmented_result = segment_image(img)\n",
    "print(\"ğŸ§© Segmented Image Output:\")\n",
    "display(segmented_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c4e4e4-a9c7-4377-9a86-d5356c477781",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ“¦ Imports\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "# ğŸ“‚ Step 1: Load image\n",
    "img_path = \"images/demo.jpg\"\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(img_path):\n",
    "    raise FileNotFoundError(f\"âŒ Image not found at {img_path}\")\n",
    "\n",
    "# Open the image\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "print(\"ğŸ–¼ï¸ Original Image:\")\n",
    "display(img)\n",
    "\n",
    "# ğŸ§© Step 2: Define segmentation function\n",
    "def segment_image(img):\n",
    "    model = deeplabv3_resnet101(pretrained=True)\n",
    "    model.eval()\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize((256, 256)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    input_tensor = preprocess(img).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor)['out'][0]\n",
    "        output_predictions = output.argmax(0)\n",
    "\n",
    "    segmented_img = Image.fromarray(output_predictions.byte().cpu().numpy())\n",
    "    return segmented_img\n",
    "\n",
    "# ğŸ§© Step 3: Segment and display\n",
    "segmented_result = segment_image(img)\n",
    "print(\"ğŸ§© Segmented Image Output:\")\n",
    "display(segmented_result)\n",
    "\n",
    "# ğŸ“ Step 4: Define dummy caption generation function\n",
    "def generate_caption(image_pil):\n",
    "    # Dummy logic â€“ replace with real captioning model later\n",
    "    return \"A cat sitting on a blanket.\"  # This is just a placeholder caption\n",
    "\n",
    "# ğŸ“ Step 5: Generate and display caption\n",
    "caption = generate_caption(img)\n",
    "print(f\"ğŸ“ Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d720b940-fee4-4b96-97c7-615845233a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5348143a-1dc9-498d-9284-f2d4af0c50cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "# Load processor and model only once\n",
    "processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "\n",
    "# Define caption generation function\n",
    "def generate_caption(image_pil):\n",
    "    inputs = processor(image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs)\n",
    "    caption = processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e5e538-c371-412d-af60-9ae5b45a398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and show caption\n",
    "caption = generate_caption(img)\n",
    "print(\"ğŸ“ Generated Caption:\", caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed219d5-fe0c-4640-b370-11d723682ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Resize segmented image to match original size (if needed)\n",
    "segmented_result_resized = segmented_result.resize(img.size)\n",
    "\n",
    "# Plot both images side by side with caption\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Original Image\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Segmented Image\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(segmented_result_resized)\n",
    "plt.title(\"Segmented Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "# Show everything\n",
    "plt.suptitle(f\"ğŸ“ Caption: {caption}\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edbc5e14-a1eb-42fc-abf7-198d8f9acfc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmented_result_resized.save(\"output/segmented_image.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7768f6-d816-4d65-98ea-15f1d57e443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/caption.txt\", \"w\") as f:\n",
    "    f.write(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5cfe43-cac0-4eb7-97df-bd749a07f7d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"output\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb25b66-7e09-46bd-a419-9270efeccef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder = \"images/\"\n",
    "for filename in os.listdir(image_folder):\n",
    "    if filename.endswith(\".jpg\") or filename.endswith(\".png\"):\n",
    "        path = os.path.join(image_folder, filename)\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        caption = generate_caption(img)\n",
    "        segmented = segment_image(img)\n",
    "        segmented_resized = segmented.resize(img.size)\n",
    "\n",
    "        # Save results\n",
    "        segmented_resized.save(f\"output/{filename}_segmented.png\")\n",
    "        with open(f\"output/{filename}_caption.txt\", \"w\") as f:\n",
    "            f.write(caption)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68bc480-84e9-4d40-b906-0ef5b9fc7fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b43261-ac14-43db-a79c-39603d3b3841",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from PIL import Image\n",
    "\n",
    "st.title(\"Image Segmentation + Captioning\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"Upload an image\", type=[\"jpg\", \"png\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    img = Image.open(uploaded_file).convert(\"RGB\")\n",
    "    st.image(img, caption=\"Original Image\")\n",
    "\n",
    "    caption = generate_caption(img)\n",
    "    segmented = segment_image(img).resize(img.size)\n",
    "\n",
    "    st.image(segmented, caption=\"Segmented Image\")\n",
    "    st.write(f\"ğŸ“ Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71628da5-c765-4249-b4bb-95c48aaeea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamlit run app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c2b56-da5a-434e-832e-fea25d666479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "\n",
    "reference = [[\"a\", \"cat\", \"is\", \"on\", \"the\", \"bed\"]]  # Must be list of list of tokens\n",
    "candidate = caption.lower().split()\n",
    "\n",
    "# Use smoothing to avoid 0 for low-overlap\n",
    "smoothie = SmoothingFunction().method4\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=(0.5, 0.5), smoothing_function=smoothie)\n",
    "print(\"âœ… BLEU Score (with smoothing):\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd09b6dc-ac7c-40a5-bcb1-3b596c01aa3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Resize segmented image to match original\n",
    "segmented_result_resized = segmented_result.resize(img.size)\n",
    "\n",
    "# Show everything together\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(segmented_result_resized)\n",
    "plt.title(\"Segmented Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.suptitle(f\"ğŸ“ Generated Caption: {caption}\\nğŸ“Š BLEU Score: {round(score, 4)}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d284236-d378-4bbc-bad3-c37fc76f09aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the path\n",
    "img_path = \"images/another_sample.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84338b2e-34b0-43f9-80c2-a085fcb8ea71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… Save caption with BLEU score to a text file (with UTF-8 encoding)\n",
    "with open(\"output/generated_caption.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"ğŸ“ Caption: {caption}\\n\")\n",
    "    f.write(f\"ğŸ“Š BLEU Score: {score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73f6278-a05a-49ce-bb16-1743add80d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.segmentation import deeplabv3_resnet101, DeepLabV3_ResNet101_Weights\n",
    "\n",
    "# Load with recommended weights\n",
    "weights = DeepLabV3_ResNet101_Weights.DEFAULT\n",
    "model = deeplabv3_resnet101(weights=weights)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b58f285-b7cb-4590-b3d0-d0e3e6aad7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114e62eb-f3d1-405d-9f8f-01037ec93017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captioning model (do NOT confuse with segmentation)\n",
    "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd0ac1f-4e92-43e8-8981-6298f2e80eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_pil):\n",
    "    inputs = caption_processor(image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = caption_model.generate(**inputs)\n",
    "    caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb711017-ddbe-4dbb-97f7-4eba0204e847",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_folder = \"images/\"\n",
    "output_folder = \"output/\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n",
    "\n",
    "for image_file in image_files:\n",
    "    print(f\"\\nğŸ”„ Processing: {image_file}\")\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    # Segment\n",
    "    segmented_result = segment_image(img)\n",
    "    segmented_result_resized = segmented_result.resize(img.size)\n",
    "\n",
    "    # Caption\n",
    "    caption = generate_caption(img)\n",
    "\n",
    "    # Save segmented image\n",
    "    seg_filename = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_segmented.jpg\")\n",
    "    segmented_result_resized.save(seg_filename)\n",
    "\n",
    "    # Save caption and BLEU score\n",
    "    reference = [[\"a\", \"cat\", \"is\", \"on\", \"the\", \"bed\"]]  # Update or automate this for batch\n",
    "    candidate = caption.lower().split()\n",
    "    smoothie = SmoothingFunction().method4\n",
    "    score = sentence_bleu(reference, candidate, weights=(0.5, 0.5), smoothing_function=smoothie)\n",
    "\n",
    "    text_filename = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_caption.txt\")\n",
    "    with open(text_filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(f\"ğŸ“ Caption: {caption}\\n\")\n",
    "        f.write(f\"ğŸ“Š BLEU Score: {score}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a16676-91f2-4df2-b1a6-e3f7fc6b1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "image_folder = \"images/\"\n",
    "output_folder = \"output/\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58ef204-6bbc-4c15-96f3-fd45471fab2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867cad3-7baf-45cb-8887-6522f47e9495",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
    "caption_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b9916c-ec46-4c6f-b990-303e8bf769f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_caption(image_pil):\n",
    "    inputs = caption_processor(image_pil, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        out = caption_model.generate(**inputs)\n",
    "    caption = caption_processor.decode(out[0], skip_special_tokens=True)\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2c250-faf9-4909-9c58-1d429423f9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_image(img):\n",
    "    # Dummy segmentation: convert to grayscale (for testing only)\n",
    "    return img.convert(\"L\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d68e4b-cc06-4cd5-bd76-e5f5d6b2aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_file in image_files:\n",
    "    print(f\"\\nğŸ”„ Processing: {image_file}\")\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    # Segment\n",
    "    segmented_result = segment_image(img)\n",
    "    segmented_result_resized = segmented_result.resize(img.size)\n",
    "    \n",
    "    # Caption\n",
    "    caption = generate_caption(img)\n",
    "\n",
    "    # Save segmented image\n",
    "    seg_filename = os.path.join(output_folder, f'{os.path.splitext(image_file)[0]}_segmented.jpg')\n",
    "    segmented_result_resized.save(seg_filename)\n",
    "\n",
    "    # Save caption\n",
    "    caption_filename = os.path.join(output_folder, f'{os.path.splitext(image_file)[0]}_caption.txt')\n",
    "    with open(caption_filename, \"w\") as f:\n",
    "        f.write(caption)\n",
    "    \n",
    "    print(f\"âœ… Saved segmented image: {seg_filename}\")\n",
    "    print(f\"ğŸ“ Caption: {caption}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776bc1cf-30f9-4c35-b83b-ea5f7393d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "    print(f\"\\nğŸ–¼ï¸ Original Image: {image_file}\")\n",
    "    display(img)\n",
    "\n",
    "    caption = generate_caption(img)\n",
    "    print(f\"ğŸ“ Caption: {caption}\")\n",
    "\n",
    "    segmented = segment_image(img)\n",
    "    display(segmented)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4853e4b-9479-484a-823c-1a8f95779361",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"captions.txt\", \"w\") as f:\n",
    "    f.write(\"image1.jpg|a cat is sitting on the bed\\n\")\n",
    "    f.write(\"image2.jpg|a man is playing football\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e014aee-183f-4e0e-9c32-c6c67f690dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    seg_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_segmented.jpg\")\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    seg_img = Image.open(seg_path)\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    axes[0].imshow(img)\n",
    "    axes[0].set_title(f\"Original: {image_file}\")\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    axes[1].imshow(seg_img)\n",
    "    axes[1].set_title(\"Segmented Output\")\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Show the caption\n",
    "    print(\"ğŸ“ Caption:\", generate_caption(img))\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae7e3ce-e625-4620-8e2e-3df41e0cc666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "from torchvision.models.detection import maskrcnn_resnet50_fpn\n",
    "\n",
    "model = maskrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e375f91a-d263-4dc5-adf9-b659b0633469",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "transform = T.Compose([T.ToTensor()])\n",
    "\n",
    "def segment_and_save(image_path, output_path):\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    img_tensor = transform(img).unsqueeze(0)  # add batch dim\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(img_tensor)[0]\n",
    "\n",
    "    # Create a blank mask and combine masks with confidence > threshold\n",
    "    masks = prediction['masks']\n",
    "    threshold = 0.5\n",
    "    combined_mask = torch.zeros_like(masks[0][0])\n",
    "\n",
    "    for i in range(len(masks)):\n",
    "        if prediction['scores'][i] > threshold:\n",
    "            combined_mask = torch.maximum(combined_mask, masks[i][0])\n",
    "\n",
    "    combined_mask = combined_mask.numpy()\n",
    "    combined_mask = (combined_mask > 0.5).astype(np.uint8) * 255  # binary mask\n",
    "\n",
    "    # Convert mask to an RGB image with colormap\n",
    "    from matplotlib import cm\n",
    "    colormap = cm.viridis(combined_mask / 255.0)\n",
    "    seg_image = Image.fromarray((colormap[:, :, :3] * 255).astype(np.uint8))\n",
    "    seg_image.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f98cc9-f144-4db3-b8b4-53a15fa23559",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_folder = \"images/\"\n",
    "output_folder = \"output/\"\n",
    "\n",
    "# Get all image filenames in the folder\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d7b0d06-8580-4c9d-b242-9c648573cdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image_file in image_files:\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abbeee6-20d4-4cd9-b56a-52ebf1e340eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"final_output\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c699cbcb-c39e-4067-9796-3978c050301c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "# âœ… Make sure the output folder exists\n",
    "os.makedirs(\"final_output\", exist_ok=True)\n",
    "\n",
    "# Dummy caption generator\n",
    "def generate_caption(image):\n",
    "    return \"This is a placeholder caption\"\n",
    "\n",
    "# Assuming image_files, image_folder, and output_folder are already defined\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    seg_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_segmented.jpg\")\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    seg_img = Image.open(seg_path)\n",
    "\n",
    "    caption = generate_caption(img)\n",
    "\n",
    "    combined = Image.new(\"RGB\", (img.width + seg_img.width, max(img.height, seg_img.height)))\n",
    "    combined.paste(img, (0, 0))\n",
    "    combined.paste(seg_img, (img.width, 0))\n",
    "\n",
    "    draw = ImageDraw.Draw(combined)\n",
    "    draw.text((10, combined.height - 30), f\"Caption: {caption}\", fill=\"white\")\n",
    "\n",
    "    output_path = os.path.join(\"final_output\", f\"{os.path.splitext(image_file)[0]}_final.jpg\")\n",
    "    combined.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d0083-956b-47f7-95bc-e2e72541d3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir('.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "554b8289-6053-4ef0-8209-e697822f9798",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import generate_caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "908c4627-4acb-48cd-aa90-bcc638d3b7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "image_folder = \"images\"\n",
    "output_folder = \"output\"\n",
    "image_files = [f for f in os.listdir(image_folder) if f.endswith(\".jpg\") or f.endswith(\".png\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "531a799c-2553-442c-b723-a10f98a896cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"final_output\", exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "468d4d2e-61d8-4ed5-a6e1-f9c38d8bff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "for image_file in image_files:\n",
    "    img_path = os.path.join(image_folder, image_file)\n",
    "    seg_path = os.path.join(output_folder, f\"{os.path.splitext(image_file)[0]}_segmented.jpg\")\n",
    "\n",
    "    img = Image.open(img_path)\n",
    "    seg_img = Image.open(seg_path)\n",
    "\n",
    "    caption = generate_caption(img)  # âœ… Now this will work\n",
    "\n",
    "    combined = Image.new(\"RGB\", (img.width + seg_img.width, max(img.height, seg_img.height)))\n",
    "    combined.paste(img, (0, 0))\n",
    "    combined.paste(seg_img, (img.width, 0))\n",
    "\n",
    "    draw = ImageDraw.Draw(combined)\n",
    "    draw.text((10, combined.height - 30), f\"Caption: {caption}\", fill=\"white\")\n",
    "\n",
    "    output_path = os.path.join(\"final_output\", f\"{os.path.splitext(image_file)[0]}_final.jpg\")\n",
    "    combined.save(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd95990-e0fd-42b5-a7ae-71c72368b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from caption_model import Captioner  # your model class\n",
    "from utils import Vocabulary          # your vocab class\n",
    "import pickle\n",
    "import os\n",
    "from flask import Flask, render_template, request, send_from_directory\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torchvision import models, transforms\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision\n",
    "\n",
    "# ========== Model Setup ========== #\n",
    "\n",
    "# Device setup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load vocabulary\n",
    "with open('vocab.pkl', 'rb') as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "# Load Captioning model (âš ï¸ Fix: no args passed here)\n",
    "model = Captioner()\n",
    "model.load_state_dict(torch.load('model.pth', map_location=device))\n",
    "model = model.to(device).eval()\n",
    "\n",
    "# ResNet feature extractor (remove classification layer)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "modules = list(resnet.children())[:-1]\n",
    "resnet = torch.nn.Sequential(*modules).to(device).eval()\n",
    "\n",
    "# Load DeepLabV3 for segmentation\n",
    "segmentation_model = torchvision.models.segmentation.deeplabv3_resnet101(weights=\"DEFAULT\")\n",
    "segmentation_model = segmentation_model.to(device).eval()\n",
    "\n",
    "# ========== Flask App Setup ========== #\n",
    "\n",
    "app = Flask(__name__)\n",
    "UPLOAD_FOLDER = \"static/uploads\"\n",
    "OUTPUT_FOLDER = \"static/outputs\"\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)\n",
    "\n",
    "# ========== Function: Generate Caption ========== #\n",
    "\n",
    "def generate_caption(image):\n",
    "    image = image.convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = resnet(image_tensor).squeeze(0)\n",
    "        caption = model.generate_caption(features.unsqueeze(0), vocab)\n",
    "\n",
    "    return caption\n",
    "\n",
    "# ========== Function: Segment Image ========== #\n",
    "\n",
    "def segment_image(image, save_path):\n",
    "    image = image.convert(\"RGB\")\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = segmentation_model(image_tensor)['out']\n",
    "        mask = torch.argmax(output.squeeze(), dim=0).cpu().numpy()\n",
    "\n",
    "    # Create color overlay mask (e.g., green for class 15 = person)\n",
    "    color_mask = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n",
    "    color_mask[mask == 15] = [0, 255, 0]\n",
    "\n",
    "    # Convert original image to OpenCV format\n",
    "    image_cv = np.array(image)\n",
    "    image_cv = cv2.cvtColor(image_cv, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    # Blend image and mask\n",
    "    blended = cv2.addWeighted(image_cv, 0.7, color_mask, 0.3, 0)\n",
    "    cv2.imwrite(save_path, blended)\n",
    "\n",
    "# ========== Routes ========== #\n",
    "\n",
    "@app.route('/', methods=['GET', 'POST'])\n",
    "def index():\n",
    "    if request.method == 'POST':\n",
    "        uploaded_file = request.files['image']\n",
    "        if uploaded_file.filename != '':\n",
    "            # Save uploaded image\n",
    "            original_filename = uploaded_file.filename\n",
    "            original_path = os.path.join(UPLOAD_FOLDER, original_filename)\n",
    "            uploaded_file.save(original_path)\n",
    "\n",
    "            # Open and process image\n",
    "            img = Image.open(original_path)\n",
    "\n",
    "            # Generate caption\n",
    "            caption = generate_caption(img)\n",
    "\n",
    "            # Segment and save output\n",
    "            segmented_filename = original_filename.rsplit('.', 1)[0] + '_segmented.jpg'\n",
    "            segmented_path = os.path.join(OUTPUT_FOLDER, segmented_filename)\n",
    "            segment_image(img, segmented_path)\n",
    "\n",
    "            # Render output page\n",
    "            return render_template('index.html',\n",
    "                                   caption=caption,\n",
    "                                   original_path=f\"uploads/{original_filename}\",\n",
    "                                   segmented_path=f\"outputs/{segmented_filename}\")\n",
    "    return render_template('index.html')\n",
    "\n",
    "@app.route('/static/uploads/<filename>')\n",
    "def uploaded_file(filename):\n",
    "    return send_from_directory(UPLOAD_FOLDER, filename)\n",
    "\n",
    "@app.route('/static/outputs/<filename>')\n",
    "def segmented_file(filename):\n",
    "    return send_from_directory(OUTPUT_FOLDER, filename)\n",
    "\n",
    "# ========== Start Server ========== #\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82cc283-3248-486c-be14-c694b93e5d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "<!DOCTYPE html>\n",
    "<html lang=\"en\">\n",
    "<head>\n",
    "    <meta charset=\"UTF-8\">\n",
    "    <title>Smart Image Captioning & Segmentation</title>\n",
    "\n",
    "    <!-- Bootstrap & Icons -->\n",
    "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n",
    "    <link href=\"https://cdn.jsdelivr.net/npm/bootstrap-icons@1.10.5/font/bootstrap-icons.css\" rel=\"stylesheet\">\n",
    "\n",
    "    <style>\n",
    "        body {\n",
    "            background-color: #f9f9f9;\n",
    "        }\n",
    "        .card {\n",
    "            border-radius: 15px;\n",
    "        }\n",
    "        .caption-box {\n",
    "            background-color: #f1f1f1;\n",
    "            padding: 10px;\n",
    "            border-radius: 8px;\n",
    "            font-size: 1.1rem;\n",
    "            color: #333;\n",
    "        }\n",
    "        .form-control {\n",
    "            padding: 14px;\n",
    "            font-size: 1.1rem;\n",
    "        }\n",
    "        .btn-success {\n",
    "            font-size: 1.2rem;\n",
    "            padding: 12px 28px;\n",
    "        }\n",
    "    </style>\n",
    "</head>\n",
    "<body>\n",
    "\n",
    "<div class=\"container mt-5\">\n",
    "    <div class=\"card shadow-lg p-4\">\n",
    "        <h2 class=\"text-center mb-4\"><i class=\"bi bi-stars\"></i> Smart Image Analyzer</h2>\n",
    "\n",
    "        <!-- Upload Form -->\n",
    "        <form method=\"POST\" action=\"/\" enctype=\"multipart/form-data\" id=\"upload-form\">\n",
    "            <div class=\"mb-3 text-center\">\n",
    "                <input class=\"form-control\" type=\"file\" name=\"image\" id=\"imageInput\" required>\n",
    "            </div>\n",
    "            <div class=\"text-center\">\n",
    "                <button class=\"btn btn-success px-4\" type=\"submit\">\n",
    "                    <i class=\"bi bi-upload\"></i> Upload & Analyze\n",
    "                </button>\n",
    "            </div>\n",
    "        </form>\n",
    "\n",
    "        <!-- Loading Spinner -->\n",
    "        <div class=\"text-center mt-4\" id=\"loading\" style=\"display: none;\">\n",
    "            <div class=\"spinner-border text-success\" role=\"status\"></div>\n",
    "            <p class=\"mt-2\">Processing image...</p>\n",
    "        </div>\n",
    "\n",
    "        {% if caption %}\n",
    "        <!-- Results -->\n",
    "        <div class=\"mt-5 row\">\n",
    "            <div class=\"col-md-6 text-center\">\n",
    "                <h5>ğŸ–¼ï¸ Original Image</h5>\n",
    "                <img src=\"{{ original_path }}\" class=\"img-fluid rounded shadow mb-2\">\n",
    "                <div class=\"caption-box\"><strong>ğŸ“ Caption:</strong> {{ caption }}</div>\n",
    "            </div>\n",
    "            <div class=\"col-md-6 text-center\">\n",
    "                <h5>ğŸ¯ Segmented Output</h5>\n",
    "                <img src=\"{{ segmented_path }}\" class=\"img-fluid rounded shadow\">\n",
    "            </div>\n",
    "        </div>\n",
    "        {% endif %}\n",
    "    </div>\n",
    "</div>\n",
    "\n",
    "<script>\n",
    "    const form = document.getElementById('upload-form');\n",
    "    form.addEventListener('submit', () => {\n",
    "        document.getElementById('loading').style.display = 'block';\n",
    "    });\n",
    "</script>\n",
    "\n",
    "</body>\n",
    "</html>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5b1124-3ee0-4ddb-b585-3401db60e3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "body {\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "}\n",
    "\n",
    ".card-header {\n",
    "    font-size: 1rem;\n",
    "}\n",
    "\n",
    ".btn {\n",
    "    transition: all 0.3s ease;\n",
    "}\n",
    "\n",
    ".btn:hover {\n",
    "    transform: scale(1.05);\n",
    "    box-shadow: 0 0 15px rgba(0, 0, 0, 0.15);\n",
    "}\n",
    "\n",
    "input[type=\"file\"] {\n",
    "    cursor: pointer;\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05d9dd8-73ac-47be-9264-a729c6470763",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create the static directory if it doesn't exist\n",
    "os.makedirs(\"static\", exist_ok=True)\n",
    "\n",
    "# Create the styles.css file inside static/\n",
    "with open(\"static/styles.css\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "body {\n",
    "    background: linear-gradient(to right, #ece9e6, #ffffff);\n",
    "    font-family: 'Segoe UI', sans-serif;\n",
    "}\n",
    "\n",
    ".card {\n",
    "    border-radius: 1rem;\n",
    "}\n",
    "\n",
    ".caption-box {\n",
    "    background-color: #f0f0f0;\n",
    "    border-radius: 8px;\n",
    "    padding: 10px;\n",
    "    font-size: 1.1em;\n",
    "}\n",
    "    \"\"\")\n",
    "print(\"âœ… static/styles.css created successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a321336-3445-46ab-8dd6-64a222547c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build_vocab.py\n",
    "\n",
    "from utils import Vocabulary\n",
    "import pickle\n",
    "\n",
    "captions_file = \"captions.txt\"\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Add special tokens\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>')\n",
    "vocab.add_word('<end>')\n",
    "vocab.add_word('<unk>')\n",
    "\n",
    "with open(captions_file, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "for line in lines:\n",
    "    if \"|\" not in line:\n",
    "        continue\n",
    "    _, caption = line.strip().split(\"|\")\n",
    "    tokens = caption.lower().strip().split()\n",
    "    for token in tokens:\n",
    "        vocab.add_word(token)\n",
    "\n",
    "# Save vocab\n",
    "with open(\"vocab.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vocab, f)\n",
    "\n",
    "print(\"âœ… vocab.pkl created with\", len(vocab), \"words.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143f0f82-69f5-4400-9135-7887269579ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "import os\n",
    "import torchvision.transforms as transforms\n",
    "import pickle\n",
    "\n",
    "class CaptionDataset(Dataset):\n",
    "    def __init__(self, caption_file, image_folder, vocab_path, transform=None):\n",
    "        with open(vocab_path, \"rb\") as f:\n",
    "            self.vocab = pickle.load(f)\n",
    "\n",
    "        with open(caption_file, \"r\") as f:\n",
    "            self.data = [line.strip().split(\"|\") for line in f if \"|\" in line]\n",
    "\n",
    "        self.image_folder = image_folder\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name, caption = self.data[idx]\n",
    "        img_path = os.path.join(self.image_folder, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        image = self.transform(image)\n",
    "\n",
    "        tokens = caption.lower().strip().split()\n",
    "        caption_idx = [self.vocab.word2idx[\"<start>\"]]\n",
    "        caption_idx += [self.vocab.word2idx.get(word, self.vocab.word2idx[\"<unk>\"]) for word in tokens]\n",
    "        caption_idx.append(self.vocab.word2idx[\"<end>\"])\n",
    "\n",
    "        return image, torch.tensor(caption_idx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c28ce4c-aee4-4a76-b403-3c64fa2b4a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Loop\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch in dataloader:\n",
    "        images, captions = zip(*batch)\n",
    "\n",
    "        # Pad captions\n",
    "        lengths = [len(cap) for cap in captions]\n",
    "        max_len = max(lengths)\n",
    "        padded = torch.zeros(len(captions), max_len).long()\n",
    "        for i, cap in enumerate(captions):\n",
    "            padded[i, :len(cap)] = cap\n",
    "\n",
    "        images = torch.stack(images).to(device)\n",
    "        captions = padded.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            features = resnet(images).squeeze()\n",
    "\n",
    "        outputs = captioner(features, captions)  # (B, T, V)\n",
    "\n",
    "        # Align dimensions for loss\n",
    "        output_dim = outputs.size(-1)\n",
    "        outputs = outputs[:, 1:, :].contiguous().view(-1, output_dim)  # shift outputs\n",
    "        targets = captions[:, 1:].contiguous().view(-1)                # shift targets\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{EPOCHS}], Loss: {loss.item():.4f}\")\n",
    "    torch.save(captioner.state_dict(), \"model.pth\")\n",
    "print(\"âœ… model.pth saved successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84209da-b52a-4aa4-a256-b7ba3d2ecc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Folder where images are stored\n",
    "image_folder = \"images\"\n",
    "os.makedirs(image_folder, exist_ok=True)\n",
    "\n",
    "# List of missing image names (based on captions.txt)\n",
    "missing_images = [\"image2.jpg\", \"image3.jpg\", \"image4.jpg\"]  # update this list\n",
    "\n",
    "# Create dummy 224x224 white images\n",
    "for img_name in missing_images:\n",
    "    img_path = os.path.join(image_folder, img_name)\n",
    "    if not os.path.exists(img_path):\n",
    "        img = Image.new('RGB', (224, 224), color=(255, 255, 255))\n",
    "        img.save(img_path)\n",
    "        print(f\"âœ… Created dummy: {img_name}\")\n",
    "    else:\n",
    "        print(f\"âœ… Already exists: {img_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a53219-7450-4922-a093-1849e813f1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "os.makedirs(\"images\", exist_ok=True)\n",
    "\n",
    "img_path = os.path.join(\"images\", \"image1.jpg\")\n",
    "dummy_img = Image.new(\"RGB\", (224, 224), color=(255, 255, 255))\n",
    "dummy_img.save(img_path)\n",
    "print(\"âœ… Created dummy: image1.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee115618-086f-482b-8f5e-69ef3a3a6d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"vocab.pkl\", \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "print(\"Vocab size:\", len(vocab))\n",
    "print(\"Max index:\", max(vocab.word2idx.values()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce996ea1-ffea-4ef1-8dfe-9897a1fe6026",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"model.pth\"))  # Should print: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8fc680-c6e2-4151-982b-d022160789b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!python app.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028eef6-ea3e-46eb-aedc-b2e39039e4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae2376-02a3-4e55-920f-70186b268e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
